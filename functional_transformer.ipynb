{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42009302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax.sharding import AxisType, NamedSharding, PartitionSpec as P\n",
    "import optax\n",
    "\n",
    "from sws import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01547740",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update('jax_num_cpu_devices', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bfd923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh=Mesh(axis_sizes=(8,), axis_names=('data',), axis_types=(Explicit,))\n"
     ]
    }
   ],
   "source": [
    "mesh = jax.make_mesh((8,), (\"data\",), (AxisType.Explicit,))\n",
    "jax.set_mesh(mesh)\n",
    "print(f\"{mesh=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de951bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARDING_RULES = {\n",
    "    \"dp\": {\n",
    "        \"batch\": \"data\",\n",
    "        \"act_seq\": None,\n",
    "        \"act_vocab\": None,\n",
    "        \"act_embed\": None,\n",
    "        \"act_intermediate\": None,\n",
    "        \"act_q\": None,\n",
    "        \"act_kv\": None,\n",
    "        \"model_seq\": None,\n",
    "        \"model_vocab\": None,\n",
    "        \"model_embed\": None,\n",
    "        \"model_intermediate\": None,\n",
    "        \"model_q\": None,\n",
    "        \"model_kv\": None,\n",
    "    },\n",
    "    \"fsdp\": {\n",
    "        \"batch\": \"data\",\n",
    "        \"act_seq\": None,\n",
    "        \"act_vocab\": None,\n",
    "        \"act_embed\": None,\n",
    "        \"act_intermediate\": None,\n",
    "        \"act_q\": None,\n",
    "        \"act_kv\": None,\n",
    "        \"model_seq\": None,\n",
    "        \"model_vocab\": \"data\",\n",
    "        \"model_embed\": None,\n",
    "        \"model_intermediate\": \"data\",\n",
    "        \"model_q\": None,\n",
    "        \"model_kv\": None,\n",
    "        \"model_head\": \"data\",\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "_current_strategy = \"fsdp\"\n",
    "\n",
    "\n",
    "def logical_to_physical(logical_axes):\n",
    "    rules = SHARDING_RULES[_current_strategy]\n",
    "    return P(*[rules.get(axis, None) for axis in logical_axes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c088796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class AttentionWeights:\n",
    "  q_proj: jax.Array\n",
    "  k_proj: jax.Array\n",
    "  v_proj: jax.Array\n",
    "  o_proj: jax.Array\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class MLPWeights:\n",
    "  up_proj: jax.Array\n",
    "  down_proj: jax.Array\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class LayerWeights:\n",
    "  attention_weights: AttentionWeights\n",
    "  mlp_weights: MLPWeights\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class ModelWeights:\n",
    "  embed: jax.Array\n",
    "  layer_weights: List[LayerWeights]\n",
    "  unembed: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735fea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(x, eps = 1e-6):\n",
    "  return (x * jax.lax.rsqrt(jnp.mean(x.astype(jnp.float32) ** 2, axis=-1, keepdims=True) + eps)).astype(x.dtype)\n",
    "\n",
    "\n",
    "def precompute_rope_embeddings(seq_len, head_dim, base):\n",
    "  channel_range = jnp.arange(0, head_dim, 2, dtype=jnp.float32)\n",
    "  inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "  t = jnp.arange(seq_len, dtype=jnp.float32)\n",
    "  freqs = jnp.outer(t, inv_freq)\n",
    "  cos, sin = jnp.cos(freqs), jnp.sin(freqs)\n",
    "  cos, sin = cos.astype(jnp.bfloat16), sin.astype(jnp.bfloat16)\n",
    "  cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
    "  return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    H = x.shape[-1] // 2\n",
    "    x1, x2 = x[..., :H], x[..., H:]\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return jnp.concat([y1, y2], axis=-1)\n",
    "  \n",
    "\n",
    "def attention(x, w: AttentionWeights, cos, sin):\n",
    "  # B = batch size\n",
    "  # D = embedding dimension\n",
    "  # S = length of the key/value (source)\n",
    "  # T = length of the query (target)\n",
    "  # N = number of attention heads\n",
    "  # H = dimensions of each attention head\n",
    "  # K = number of key/value heads\n",
    "  # G = number of groups, which equals to N // K\n",
    "  \n",
    "  T = x.shape[1]\n",
    "  H = w.q_proj.shape[2]\n",
    "  G = w.q_proj.shape[1] // w.k_proj.shape[1]\n",
    "  \n",
    "  q = jnp.einsum(\n",
    "    \"BTD, DNH -> BTNH\", x, w.q_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "    )\n",
    "  k = jnp.einsum(\n",
    "    \"BSD, DKH -> BSKH\", x, w.k_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_kv\", \"act_head\"))\n",
    "  )\n",
    "  v = jnp.einsum(\n",
    "    \"BSD, DKH -> BSKH\", x, w.v_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_kv\", \"act_head\"))\n",
    "  )\n",
    "\n",
    "  q = apply_rope(q, cos, sin)\n",
    "  k = apply_rope(k, cos, sin)\n",
    "\n",
    "  q = rms_norm(q)\n",
    "  k = rms_norm(k)\n",
    "\n",
    "  k = jnp.repeat(\n",
    "    k, G, axis=2,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "  )\n",
    "  v = jnp.repeat(\n",
    "    v, G, axis=2,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "  )\n",
    "  \n",
    "  logits = jnp.einsum(\n",
    "    \"BTNH, BSNH -> BNTS\", q, k,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_q\", \"act_seq\", \"act_seq\"))\n",
    "  )\n",
    "  logits *= jax.lax.rsqrt(jnp.array(H, dtype=jnp.bfloat16))\n",
    "  causal_mask = jnp.tril(jnp.ones((T, T,), dtype=jnp.bfloat16))\n",
    "  masked_logits = jnp.where(causal_mask, logits, jnp.array(float(\"-inf\"), dtype=jnp.bfloat16))\n",
    "  probs = jax.nn.softmax(masked_logits.astype(jnp.float32), axis=-1).astype(jnp.bfloat16)\n",
    "  encoded = jnp.einsum(\n",
    "    \"BNTS, BSNH -> BTNH\", probs, v,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "  )\n",
    "  out = jnp.einsum(\n",
    "    \"BTNH, NHD -> BTD\", encoded, w.o_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_embed\"))\n",
    "  )\n",
    "\n",
    "  return out\n",
    "\n",
    "def mlp(x, w: MLPWeights):\n",
    "  intermediate = jnp.matmul(\n",
    "    x, w.up_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_intermediate\"))\n",
    "  )\n",
    "  return jnp.matmul(\n",
    "    jax.nn.silu(intermediate), w.down_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_embed\"))\n",
    "  )\n",
    "\n",
    "\n",
    "def layer(x, w: LayerWeights, cos, sin):\n",
    "  x = x + attention(rms_norm(x), w.attention_weights, cos, sin)\n",
    "  x = x + mlp(rms_norm(x), w.mlp_weights)\n",
    "  return x\n",
    "\n",
    "@jax.jit\n",
    "def forward(x, w: ModelWeights, cos, sin):\n",
    "  x = w.embed.at[x].get(out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_embed\"))).astype(jnp.bfloat16)\n",
    "  for layer_weights in w.layer_weights:\n",
    "    x = layer(x, layer_weights, cos, sin)\n",
    "  logits = jnp.matmul(\n",
    "    x, w.unembed.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_vocab\"))\n",
    "  )\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e530292",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Config()\n",
    "\n",
    "c.model.seq_len = 1024\n",
    "c.model.vocab_size = 50304\n",
    "c.model.num_layers = 1\n",
    "c.model.hidden_dim = 512\n",
    "c.model.intermediate_dim = lambda: 4 * c.model.hidden_dim\n",
    "c.model.num_attention_heads = 8\n",
    "c.model.num_key_value_heads = 8\n",
    "c.model.head_dim = lambda: c.model.hidden_dim // c.model.num_attention_heads\n",
    "c.model.rope_base = 10000\n",
    "\n",
    "c.optimizer.learning_rate = 0.0001\n",
    "c.optimizer.weight_decay = 0.01\n",
    "c.optimizer.beta1 = 0.9\n",
    "c.optimizer.beta2 = 0.999\n",
    "c.optimizer.eps = 1e-8\n",
    "\n",
    "c = c.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a3a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_weights(\n",
    "    vocab_size,\n",
    "    num_layers,\n",
    "    hidden_dim,\n",
    "    intermediate_dim,\n",
    "    num_attention_heads,\n",
    "    num_key_value_heads,\n",
    "    head_dim\n",
    "):\n",
    "    num_weight_arrays = 1 + (num_layers * 6) + 1\n",
    "    key = jax.random.key(69420)\n",
    "    key_iter = iter(jax.random.split(key, num_weight_arrays))\n",
    "    \n",
    "    init_fn = jax.nn.initializers.lecun_normal()\n",
    "    \n",
    "    embed = init_fn(\n",
    "        next(key_iter), (vocab_size, hidden_dim), dtype=jnp.float32,\n",
    "        out_sharding=logical_to_physical((\"model_vocab\", \"model_embed\"))\n",
    "    )\n",
    "    layer_weights = [\n",
    "        LayerWeights(\n",
    "            attention_weights=AttentionWeights(\n",
    "                q_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, num_attention_heads, head_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_q\", \"model_head\"))\n",
    "                ),\n",
    "                k_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, num_key_value_heads, head_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_kv\", \"model_head\"))\n",
    "                ),\n",
    "                v_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, num_key_value_heads, head_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_kv\", \"model_head\"))\n",
    "                ),\n",
    "                o_proj=init_fn(\n",
    "                    next(key_iter), (num_attention_heads, head_dim, hidden_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_q\", \"model_head\", \"model_embed\"))\n",
    "                )\n",
    "            ),\n",
    "            mlp_weights = MLPWeights(\n",
    "                up_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, intermediate_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_intermediate\"))\n",
    "                ),\n",
    "                down_proj=init_fn(\n",
    "                    next(key_iter), (intermediate_dim, hidden_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_intermediate\", \"model_embed\"))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "    ]\n",
    "    unembed = init_fn(\n",
    "        next(key_iter), (hidden_dim, vocab_size), dtype=jnp.float32,\n",
    "        out_sharding=logical_to_physical((\"model_embed\", \"model_vocab\"))\n",
    "    )\n",
    "    model_weights = ModelWeights(embed=embed, layer_weights=layer_weights, unembed=unembed)\n",
    "\n",
    "    return model_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d393b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = init_model_weights(\n",
    "    vocab_size=c.model.vocab_size,\n",
    "    num_layers=c.model.num_layers,\n",
    "    hidden_dim=c.model.hidden_dim,\n",
    "    intermediate_dim=c.model.intermediate_dim,\n",
    "    num_attention_heads=c.model.num_attention_heads,\n",
    "    num_key_value_heads=c.model.num_key_value_heads,\n",
    "    head_dim=c.model.head_dim\n",
    ")\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=c.optimizer.learning_rate,\n",
    "    weight_decay=c.optimizer.weight_decay,\n",
    "    b1=c.optimizer.beta1,\n",
    "    b2=c.optimizer.beta2,\n",
    "    eps=c.optimizer.eps,\n",
    ")\n",
    "optimizer_state = optimizer.init(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499a63a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 1024, 50304), dtype(bfloat16))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin = precompute_rope_embeddings(c.model.seq_len, c.model.head_dim, c.model.rope_base)\n",
    "x = jnp.ones((8, 1024), dtype=jnp.int32)\n",
    "logits = forward(x, model_weights, cos, sin)\n",
    "logits.shape, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ca8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(w, x, y):\n",
    "    logits = forward(x, w)\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    labels = y.reshape(-1, 1)\n",
    "    label_logits = jnp.take_along_axis(logits, labels, axis=-1)\n",
    "    log_normalizers = jax.nn.logsumexp(logits, axis=-1)\n",
    "    return jnp.mean(log_normalizers - label_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model_weights, optimizer_state, x, y):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(model_weights, x, y)\n",
    "    updates, optimizer_state = optimizer.update(grads, optimizer_state, model_weights)\n",
    "    model_weights = optax.apply_updates(model_weights, updates)\n",
    "    return model_weights, optimizer_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    model_weights, optimizer_state, loss = train_step(model_weights, optimizer_state, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c9c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
