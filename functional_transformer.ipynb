{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42009302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax.sharding import AxisType, NamedSharding, PartitionSpec as P\n",
    "import optax\n",
    "\n",
    "from sws import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01547740",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update('jax_num_cpu_devices', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bfd923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh=Mesh(axis_sizes=(8,), axis_names=('data',), axis_types=(Explicit,))\n"
     ]
    }
   ],
   "source": [
    "mesh = jax.make_mesh((8,), (\"data\",), (AxisType.Explicit,))\n",
    "jax.set_mesh(mesh)\n",
    "print(f\"{mesh=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de951bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARDING_RULES = {\n",
    "    \"dp\": {\n",
    "        \"batch\": \"data\",\n",
    "        \"act_seq\": None,\n",
    "        \"act_vocab\": None,\n",
    "        \"act_embed\": None,\n",
    "        \"act_intermediate\": None,\n",
    "        \"act_q\": None,\n",
    "        \"act_kv\": None,\n",
    "        \"model_seq\": None,\n",
    "        \"model_vocab\": None,\n",
    "        \"model_embed\": None,\n",
    "        \"model_intermediate\": None,\n",
    "        \"model_q\": None,\n",
    "        \"model_kv\": None,\n",
    "    },\n",
    "    \"fsdp\": {\n",
    "        \"batch\": \"data\",\n",
    "        \"act_seq\": None,\n",
    "        \"act_vocab\": None,\n",
    "        \"act_embed\": None,\n",
    "        \"act_intermediate\": None,\n",
    "        \"act_q\": None,\n",
    "        \"act_kv\": None,\n",
    "        \"model_seq\": None,\n",
    "        \"model_vocab\": \"data\",\n",
    "        \"model_embed\": None,\n",
    "        \"model_intermediate\": \"data\",\n",
    "        \"model_q\": None,\n",
    "        \"model_kv\": None,\n",
    "        \"model_head\": \"data\",\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "_current_strategy = \"dp\"\n",
    "\n",
    "\n",
    "def logical_to_physical(logical_axes):\n",
    "    rules = SHARDING_RULES[_current_strategy]\n",
    "    return P(*[rules.get(axis, None) for axis in logical_axes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c088796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class AttentionWeights:\n",
    "  q_proj: jax.Array\n",
    "  k_proj: jax.Array\n",
    "  v_proj: jax.Array\n",
    "  o_proj: jax.Array\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class MLPWeights:\n",
    "  up_proj: jax.Array\n",
    "  down_proj: jax.Array\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class LayerWeights:\n",
    "  attention_weights: AttentionWeights\n",
    "  mlp_weights: MLPWeights\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class ModelWeights:\n",
    "  embed: jax.Array\n",
    "  layer_weights: List[LayerWeights]\n",
    "  unembed: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735fea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(x, eps = 1e-6):\n",
    "  return (x * jax.lax.rsqrt(jnp.mean(x.astype(jnp.float32) ** 2, axis=-1, keepdims=True) + eps)).astype(x.dtype)\n",
    "\n",
    "\n",
    "def precompute_rope_embeddings(seq_len, head_dim, base):\n",
    "  channel_range = jnp.arange(0, head_dim, 2, dtype=jnp.float32)\n",
    "  inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "  t = jnp.arange(seq_len, dtype=jnp.float32)\n",
    "  freqs = jnp.outer(t, inv_freq)\n",
    "  cos, sin = jnp.cos(freqs), jnp.sin(freqs)\n",
    "  cos, sin = cos.astype(jnp.bfloat16), sin.astype(jnp.bfloat16)\n",
    "  cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
    "  return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    H = x.shape[-1] // 2\n",
    "    x1, x2 = x[..., :H], x[..., H:]\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return jnp.concat([y1, y2], axis=-1)\n",
    "  \n",
    "\n",
    "def attention(x, w: AttentionWeights, cos, sin):\n",
    "  # B = batch size\n",
    "  # D = embedding dimension\n",
    "  # S = length of the key/value (source)\n",
    "  # T = length of the query (target)\n",
    "  # N = number of attention heads\n",
    "  # H = dimensions of each attention head\n",
    "  # K = number of key/value heads\n",
    "  # G = number of groups, which equals to N // K\n",
    "  \n",
    "  T = x.shape[1]\n",
    "  H = w.q_proj.shape[2]\n",
    "  G = w.q_proj.shape[1] // w.k_proj.shape[1]\n",
    "  \n",
    "  q = jnp.einsum(\n",
    "    \"BTD, DNH -> BTNH\", x, w.q_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "    )\n",
    "  k = jnp.einsum(\n",
    "    \"BSD, DKH -> BSKH\", x, w.k_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_kv\", \"act_head\"))\n",
    "  )\n",
    "  v = jnp.einsum(\n",
    "    \"BSD, DKH -> BSKH\", x, w.v_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_kv\", \"act_head\"))\n",
    "  )\n",
    "\n",
    "  q = apply_rope(q, cos, sin)\n",
    "  k = apply_rope(k, cos, sin)\n",
    "\n",
    "  q = rms_norm(q)\n",
    "  k = rms_norm(k)\n",
    "\n",
    "  k = jnp.repeat(\n",
    "    k, G, axis=2,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "  )\n",
    "  v = jnp.repeat(\n",
    "    v, G, axis=2,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "  )\n",
    "  \n",
    "  logits = jnp.einsum(\n",
    "    \"BTNH, BSNH -> BNTS\", q, k,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_q\", \"act_seq\", \"act_seq\"))\n",
    "  )\n",
    "  logits *= jax.lax.rsqrt(jnp.array(H, dtype=jnp.bfloat16))\n",
    "  causal_mask = jnp.tril(jnp.ones((T, T,), dtype=jnp.bfloat16))\n",
    "  masked_logits = jnp.where(causal_mask, logits, jnp.array(float(\"-inf\"), dtype=jnp.bfloat16))\n",
    "  probs = jax.nn.softmax(masked_logits.astype(jnp.float32), axis=-1).astype(jnp.bfloat16)\n",
    "  encoded = jnp.einsum(\n",
    "    \"BNTS, BSNH -> BTNH\", probs, v,\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_q\", \"act_head\"))\n",
    "  )\n",
    "  out = jnp.einsum(\n",
    "    \"BTNH, NHD -> BTD\", encoded, w.o_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_embed\"))\n",
    "  )\n",
    "\n",
    "  return out\n",
    "\n",
    "def mlp(x, w: MLPWeights):\n",
    "  intermediate = jnp.matmul(\n",
    "    x, w.up_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_intermediate\"))\n",
    "  )\n",
    "  return jnp.matmul(\n",
    "    jax.nn.silu(intermediate), w.down_proj.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_embed\"))\n",
    "  )\n",
    "\n",
    "\n",
    "def layer(x, w: LayerWeights, cos, sin):\n",
    "  x = x + attention(rms_norm(x), w.attention_weights, cos, sin)\n",
    "  x = x + mlp(rms_norm(x), w.mlp_weights)\n",
    "  return x\n",
    "\n",
    "@jax.jit\n",
    "def forward(x, w: ModelWeights, cos, sin):\n",
    "  x = w.embed.at[x].get(out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_embed\"))).astype(jnp.bfloat16)\n",
    "  for layer_weights in w.layer_weights:\n",
    "    x = layer(x, layer_weights, cos, sin)\n",
    "  logits = jnp.matmul(\n",
    "    x, w.unembed.astype(jnp.bfloat16),\n",
    "    out_sharding=logical_to_physical((\"batch\", \"act_seq\", \"act_vocab\"))\n",
    "  )\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e530292",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Config()\n",
    "\n",
    "c.model.seq_len = 1024\n",
    "c.model.vocab_size = 50304\n",
    "c.model.num_layers = 1\n",
    "c.model.hidden_dim = 512\n",
    "c.model.intermediate_dim = lambda: 4 * c.model.hidden_dim\n",
    "c.model.num_attention_heads = 8\n",
    "c.model.num_key_value_heads = 8\n",
    "c.model.head_dim = lambda: c.model.hidden_dim // c.model.num_attention_heads\n",
    "c.model.rope_base = 10000\n",
    "\n",
    "c.optimizer.learning_rate = 0.0001\n",
    "c.optimizer.weight_decay = 0.01\n",
    "c.optimizer.beta1 = 0.9\n",
    "c.optimizer.beta2 = 0.999\n",
    "c.optimizer.eps = 1e-8\n",
    "\n",
    "c = c.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a3a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_weights(\n",
    "    vocab_size,\n",
    "    num_layers,\n",
    "    hidden_dim,\n",
    "    intermediate_dim,\n",
    "    num_attention_heads,\n",
    "    num_key_value_heads,\n",
    "    head_dim\n",
    "):\n",
    "    num_weight_arrays = 1 + (num_layers * 6) + 1\n",
    "    key = jax.random.key(69420)\n",
    "    key_iter = iter(jax.random.split(key, num_weight_arrays))\n",
    "    \n",
    "    init_fn = jax.nn.initializers.lecun_normal()\n",
    "    \n",
    "    embed = init_fn(\n",
    "        next(key_iter), (vocab_size, hidden_dim), dtype=jnp.float32,\n",
    "        out_sharding=logical_to_physical((\"model_vocab\", \"model_embed\"))\n",
    "    )\n",
    "    layer_weights = [\n",
    "        LayerWeights(\n",
    "            attention_weights=AttentionWeights(\n",
    "                q_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, num_attention_heads, head_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_q\", \"model_head\"))\n",
    "                ),\n",
    "                k_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, num_key_value_heads, head_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_kv\", \"model_head\"))\n",
    "                ),\n",
    "                v_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, num_key_value_heads, head_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_kv\", \"model_head\"))\n",
    "                ),\n",
    "                o_proj=init_fn(\n",
    "                    next(key_iter), (num_attention_heads, head_dim, hidden_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_q\", \"model_head\", \"model_embed\"))\n",
    "                )\n",
    "            ),\n",
    "            mlp_weights = MLPWeights(\n",
    "                up_proj=init_fn(\n",
    "                    next(key_iter), (hidden_dim, intermediate_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_embed\", \"model_intermediate\"))\n",
    "                ),\n",
    "                down_proj=init_fn(\n",
    "                    next(key_iter), (intermediate_dim, hidden_dim), dtype=jnp.float32,\n",
    "                    out_sharding=logical_to_physical((\"model_intermediate\", \"model_embed\"))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        for _ in range(num_layers)\n",
    "    ]\n",
    "    unembed = init_fn(\n",
    "        next(key_iter), (hidden_dim, vocab_size), dtype=jnp.float32,\n",
    "        out_sharding=logical_to_physical((\"model_embed\", \"model_vocab\"))\n",
    "    )\n",
    "    model_weights = ModelWeights(embed=embed, layer_weights=layer_weights, unembed=unembed)\n",
    "\n",
    "    return model_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d393b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = init_model_weights(\n",
    "    vocab_size=c.model.vocab_size,\n",
    "    num_layers=c.model.num_layers,\n",
    "    hidden_dim=c.model.hidden_dim,\n",
    "    intermediate_dim=c.model.intermediate_dim,\n",
    "    num_attention_heads=c.model.num_attention_heads,\n",
    "    num_key_value_heads=c.model.num_key_value_heads,\n",
    "    head_dim=c.model.head_dim\n",
    ")\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=c.optimizer.learning_rate,\n",
    "    weight_decay=c.optimizer.weight_decay,\n",
    "    b1=c.optimizer.beta1,\n",
    "    b2=c.optimizer.beta2,\n",
    "    eps=c.optimizer.eps,\n",
    ")\n",
    "optimizer_state = optimizer.init(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094d4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_state = (\n",
    "    jax.tree.map(lambda x: jax.sharding.reshard(x, P(\"data\",)) if x.ndim > 1 else x, optimizer_state[0]),\n",
    "    optimizer_state[1],\n",
    "    optimizer_state[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a06b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedSharding(mesh=Mesh('data': 8, axis_types=(Explicit,)), spec=PartitionSpec(None, None, None), memory_kind=device)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights.layer_weights[0].attention_weights.q_proj.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "177ac81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedSharding(mesh=Mesh('data': 8, axis_types=(Explicit,)), spec=PartitionSpec('data', None, None), memory_kind=device)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_state[0].mu.layer_weights[0].attention_weights.q_proj.sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "499a63a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 1024, 50304), dtype(bfloat16))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin = precompute_rope_embeddings(c.model.seq_len, c.model.head_dim, c.model.rope_base)\n",
    "x = jnp.ones((8, 1024), dtype=jnp.int32, out_sharding=logical_to_physical((\"batch\", \"act_seq\")))\n",
    "y = jnp.ones((8, 1024), dtype=jnp.int32, out_sharding=logical_to_physical((\"batch\", \"act_seq\")))\n",
    "logits = forward(x, model_weights, cos, sin)\n",
    "logits.shape, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a3ca8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(w, cos, sin, x, y):\n",
    "    logits = forward(x, w, cos, sin)\n",
    "    label_logits = jnp.take_along_axis(logits, y[..., jnp.newaxis], axis=-1)\n",
    "    log_normalizers = jax.nn.logsumexp(logits, axis=-1, keepdims=True)\n",
    "    return jnp.mean(log_normalizers - label_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e46ff264",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.sharding.auto_axes\n",
    "def apply_updates(model_weights, updates):\n",
    "  return jax.tree.map(lambda weights, updates: updates + weights, model_weights, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e068b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/_8vp667s3gvfxcww_plg6qwc0000gn/T/ipykernel_5555/987836271.py:2: DeprecationWarning: jax.experimental.shard_map is deprecated in v0.8.0. Used jax.shard_map instead.\n",
      "  from jax.experimental.shard_map import shard_map\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from jax.experimental.shard_map import shard_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c691adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_optimizer_update(optimizer, model_weights, optimizer_state):\n",
    "    \"\"\"Create a ZeRO-1/2 style update function with proper specs for the given structures.\"\"\"\n",
    "    \n",
    "    # Build partition specs matching the pytree structures\n",
    "    # Grads and model weights are replicated (P())\n",
    "    # Optimizer state is sharded on first axis for arrays with ndim > 1\n",
    "    grads_spec = jax.tree.map(lambda _: P(), model_weights)\n",
    "    model_spec = jax.tree.map(lambda _: P(), model_weights)\n",
    "    \n",
    "    # For optimizer state: shard arrays with ndim > 1 on first axis, replicate scalars/1D\n",
    "    def opt_state_to_spec(x):\n",
    "        if hasattr(x, 'ndim') and x.ndim > 1:\n",
    "            return P(\"data\",)\n",
    "        return P()\n",
    "    \n",
    "    opt_state_spec = jax.tree.map(opt_state_to_spec, optimizer_state)\n",
    "    \n",
    "    # Output specs: updates are replicated, optimizer state stays sharded\n",
    "    updates_spec = jax.tree.map(lambda _: P(), model_weights)\n",
    "    \n",
    "    @partial(shard_map, mesh=mesh,\n",
    "             in_specs=(grads_spec, opt_state_spec, model_spec),\n",
    "             out_specs=(updates_spec, opt_state_spec),\n",
    "             check_rep=False)\n",
    "    def _zero_update(grads, opt_state, model):\n",
    "        # Get axis info for slicing\n",
    "        axis_size = jax.lax.psum(1, \"data\")\n",
    "        axis_index = jax.lax.axis_index(\"data\")\n",
    "        \n",
    "        # Reduce-scatter gradients: each device gets 1/N of fully-reduced grads\n",
    "        def reduce_scatter_grad(g):\n",
    "            if g.ndim > 1:\n",
    "                return jax.lax.psum_scatter(g, \"data\", scatter_dimension=0, tiled=True)\n",
    "            else:\n",
    "                # For 1D arrays (scalars after vmap, biases, etc.), just reduce\n",
    "                return jax.lax.pmean(g, \"data\")\n",
    "        \n",
    "        grads_sharded = jax.tree.map(reduce_scatter_grad, grads)\n",
    "        \n",
    "        # Slice model weights to match sharded gradients (for weight decay)\n",
    "        def slice_to_shard(arr):\n",
    "            if arr.ndim > 1:\n",
    "                shard_size = arr.shape[0] // axis_size\n",
    "                return jax.lax.dynamic_slice_in_dim(arr, axis_index * shard_size, shard_size, axis=0)\n",
    "            return arr\n",
    "        \n",
    "        model_sharded = jax.tree.map(slice_to_shard, model)\n",
    "        \n",
    "        # Local optimizer update with sharded grads, sharded state, and sliced model\n",
    "        updates_sharded, new_opt_state = optimizer.update(grads_sharded, opt_state, model_sharded)\n",
    "        \n",
    "        # All-gather updates to replicate them\n",
    "        def all_gather_update(u):\n",
    "            if u.ndim > 1:\n",
    "                return jax.lax.all_gather(u, \"data\", axis=0, tiled=True)\n",
    "            else:\n",
    "                return u\n",
    "        \n",
    "        updates_full = jax.tree.map(all_gather_update, updates_sharded)\n",
    "        \n",
    "        return updates_full, new_opt_state\n",
    "    \n",
    "    return _zero_update\n",
    "\n",
    "\n",
    "# Create the ZeRO update function with the actual structures\n",
    "zero_update = make_zero_optimizer_update(optimizer, model_weights, optimizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a89f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(model_weights, optimizer_state, cos, sin, x, y):\n",
    "    print(\"model, optimizer_state\")\n",
    "    print(jax.typeof(model_weights.layer_weights[0].attention_weights.q_proj), jax.typeof(optimizer_state[0].mu.layer_weights[0].attention_weights.q_proj))\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(model_weights, cos, sin, x, y)\n",
    "    print(\"model, optimizer_state, grads\")\n",
    "    print(jax.typeof(model_weights.layer_weights[0].attention_weights.q_proj), jax.typeof(optimizer_state[0].mu.layer_weights[0].attention_weights.q_proj), jax.typeof(grads.layer_weights[0].attention_weights.q_proj))\n",
    "    # updates, optimizer_state = optimizer.update(grads, optimizer_state, model_weights)\n",
    "    updates, optimizer_state = zero_update(grads, optimizer_state, model_weights)\n",
    "    print(\"model, updates, optimizer_state\")\n",
    "    print(jax.typeof(model_weights.layer_weights[0].attention_weights.q_proj), jax.typeof(updates.layer_weights[0].attention_weights.q_proj), jax.typeof(optimizer_state[0].mu.layer_weights[0].attention_weights.q_proj))\n",
    "    model_weights = optax.apply_updates(model_weights, updates)\n",
    "    print(\"model, optimizer_state\")\n",
    "    print(jax.typeof(model_weights.layer_weights[0].attention_weights.q_proj), jax.typeof(optimizer_state[0].mu.layer_weights[0].attention_weights.q_proj))\n",
    "    return model_weights, optimizer_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d2d8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(100):\n",
    "#     model_weights, optimizer_state, loss = train_step(model_weights, optimizer_state, cos, sin,x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40107457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model, optimizer_state\n",
      "float32[512,8,64] float32[512@data,8,64]\n",
      "model, optimizer_state, grads\n",
      "float32[512,8,64] float32[512@data,8,64] float32[512,8,64]\n",
      "model, updates, optimizer_state\n",
      "float32[512,8,64] float32[512,8,64] float32[512@data,8,64]\n",
      "model, optimizer_state\n",
      "float32[512,8,64] float32[512@data,8,64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[50304,512]\u001b[39m b\u001b[35m:f32[512,8,64]\u001b[39m c\u001b[35m:f32[512,8,64]\u001b[39m d\u001b[35m:f32[512,8,64]\u001b[39m e\u001b[35m:f32[8,64,512]\u001b[39m\n",
       "    f\u001b[35m:f32[512,2048]\u001b[39m g\u001b[35m:f32[2048,512]\u001b[39m h\u001b[35m:f32[512,50304]\u001b[39m i\u001b[35m:i32[]\u001b[39m j\u001b[35m:f32[50304@data,512]\u001b[39m\n",
       "    k\u001b[35m:f32[512@data,8,64]\u001b[39m l\u001b[35m:f32[512@data,8,64]\u001b[39m m\u001b[35m:f32[512@data,8,64]\u001b[39m n\u001b[35m:f32[8@data,64,512]\u001b[39m\n",
       "    o\u001b[35m:f32[512@data,2048]\u001b[39m p\u001b[35m:f32[2048@data,512]\u001b[39m q\u001b[35m:f32[512@data,50304]\u001b[39m r\u001b[35m:f32[50304@data,512]\u001b[39m\n",
       "    s\u001b[35m:f32[512@data,8,64]\u001b[39m t\u001b[35m:f32[512@data,8,64]\u001b[39m u\u001b[35m:f32[512@data,8,64]\u001b[39m v\u001b[35m:f32[8@data,64,512]\u001b[39m\n",
       "    w\u001b[35m:f32[512@data,2048]\u001b[39m x\u001b[35m:f32[2048@data,512]\u001b[39m y\u001b[35m:f32[512@data,50304]\u001b[39m z\u001b[35m:bf16[1,1024,1,32]\u001b[39m\n",
       "    ba\u001b[35m:bf16[1,1024,1,32]\u001b[39m bb\u001b[35m:i32[8@data,1024]\u001b[39m bc\u001b[35m:i32[8@data,1024]\u001b[39m. \u001b[34;1mlet\n",
       "    \u001b[39;22mbd\u001b[35m:f32[50304,512]\u001b[39m be\u001b[35m:f32[512,8,64]\u001b[39m bf\u001b[35m:f32[512,8,64]\u001b[39m bg\u001b[35m:f32[512,8,64]\u001b[39m bh\u001b[35m:f32[8,64,512]\u001b[39m\n",
       "      bi\u001b[35m:f32[512,2048]\u001b[39m bj\u001b[35m:f32[2048,512]\u001b[39m bk\u001b[35m:f32[512,50304]\u001b[39m bl\u001b[35m:i32[]\u001b[39m bm\u001b[35m:f32[50304@data,512]\u001b[39m\n",
       "      bn\u001b[35m:f32[512@data,8,64]\u001b[39m bo\u001b[35m:f32[512@data,8,64]\u001b[39m bp\u001b[35m:f32[512@data,8,64]\u001b[39m bq\u001b[35m:f32[8@data,64,512]\u001b[39m\n",
       "      br\u001b[35m:f32[512@data,2048]\u001b[39m bs\u001b[35m:f32[2048@data,512]\u001b[39m bt\u001b[35m:f32[512@data,50304]\u001b[39m bu\u001b[35m:f32[50304@data,512]\u001b[39m\n",
       "      bv\u001b[35m:f32[512@data,8,64]\u001b[39m bw\u001b[35m:f32[512@data,8,64]\u001b[39m bx\u001b[35m:f32[512@data,8,64]\u001b[39m by\u001b[35m:f32[8@data,64,512]\u001b[39m\n",
       "      bz\u001b[35m:f32[512@data,2048]\u001b[39m ca\u001b[35m:f32[2048@data,512]\u001b[39m cb\u001b[35m:f32[512@data,50304]\u001b[39m cc\u001b[35m:bf16[]\u001b[39m = jit[\n",
       "      name=train_step\n",
       "      ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[50304,512]\u001b[39m b\u001b[35m:f32[512,8,64]\u001b[39m c\u001b[35m:f32[512,8,64]\u001b[39m d\u001b[35m:f32[512,8,64]\u001b[39m\n",
       "          e\u001b[35m:f32[8,64,512]\u001b[39m f\u001b[35m:f32[512,2048]\u001b[39m g\u001b[35m:f32[2048,512]\u001b[39m h\u001b[35m:f32[512,50304]\u001b[39m i\u001b[35m:i32[]\u001b[39m\n",
       "          j\u001b[35m:f32[50304@data,512]\u001b[39m k\u001b[35m:f32[512@data,8,64]\u001b[39m l\u001b[35m:f32[512@data,8,64]\u001b[39m m\u001b[35m:f32[512@data,8,64]\u001b[39m\n",
       "          n\u001b[35m:f32[8@data,64,512]\u001b[39m o\u001b[35m:f32[512@data,2048]\u001b[39m p\u001b[35m:f32[2048@data,512]\u001b[39m q\u001b[35m:f32[512@data,50304]\u001b[39m\n",
       "          r\u001b[35m:f32[50304@data,512]\u001b[39m s\u001b[35m:f32[512@data,8,64]\u001b[39m t\u001b[35m:f32[512@data,8,64]\u001b[39m u\u001b[35m:f32[512@data,8,64]\u001b[39m\n",
       "          v\u001b[35m:f32[8@data,64,512]\u001b[39m w\u001b[35m:f32[512@data,2048]\u001b[39m x\u001b[35m:f32[2048@data,512]\u001b[39m y\u001b[35m:f32[512@data,50304]\u001b[39m\n",
       "          z\u001b[35m:bf16[1,1024,1,32]\u001b[39m ba\u001b[35m:bf16[1,1024,1,32]\u001b[39m bb\u001b[35m:i32[8@data,1024]\u001b[39m bc\u001b[35m:i32[8@data,1024]\u001b[39m. \u001b[34;1mlet\n",
       "          \u001b[39;22mcd\u001b[35m:bf16[8@data,1024,50304]\u001b[39m ce\u001b[35m:i32[8,1024,1]\u001b[39m cf\u001b[35m:f32[8@data,1024,512]\u001b[39m cg\u001b[35m:f32[8@data,1024,1]\u001b[39m\n",
       "            ch\u001b[35m:f32[8@data,1024,1]\u001b[39m ci\u001b[35m:f32[8@data,1024,512]\u001b[39m cj\u001b[35m:bf16[512,8,64]\u001b[39m ck\u001b[35m:bf16[8@data,1024,512]\u001b[39m\n",
       "            cl\u001b[35m:bf16[512,8,64]\u001b[39m cm\u001b[35m:bf16[512,8,64]\u001b[39m cn\u001b[35m:bf16[1,1024,1,32]\u001b[39m co\u001b[35m:bf16[1,1024,1,32]\u001b[39m\n",
       "            cp\u001b[35m:f32[8@data,1024,8,64]\u001b[39m cq\u001b[35m:f32[8@data,1024,8,1]\u001b[39m cr\u001b[35m:f32[8@data,1024,8,1]\u001b[39m\n",
       "            cs\u001b[35m:f32[8@data,1024,8,64]\u001b[39m ct\u001b[35m:f32[8@data,1024,8,64]\u001b[39m cu\u001b[35m:f32[8@data,1024,8,1]\u001b[39m\n",
       "            cv\u001b[35m:f32[8@data,1024,8,1]\u001b[39m cw\u001b[35m:f32[8@data,1024,8,64]\u001b[39m cx\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m\n",
       "            cy\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m cz\u001b[35m:bf16[]\u001b[39m da\u001b[35m:bool[8@data,8,1024,1024]\u001b[39m db\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m\n",
       "            dc\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m dd\u001b[35m:f32[8@data,8,1024,1]\u001b[39m de\u001b[35m:f32[8@data,8,1024,1]\u001b[39m\n",
       "            df\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m dg\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m dh\u001b[35m:bf16[8,64,512]\u001b[39m\n",
       "            di\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m dj\u001b[35m:f32[8@data,1024,512]\u001b[39m dk\u001b[35m:f32[8@data,1024,1]\u001b[39m\n",
       "            dl\u001b[35m:f32[8@data,1024,1]\u001b[39m dm\u001b[35m:f32[8@data,1024,512]\u001b[39m dn\u001b[35m:bf16[512,2048]\u001b[39m do\u001b[35m:bf16[8@data,1024,512]\u001b[39m\n",
       "            dp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dr\u001b[35m:bf16[8@data,1024,2048]\u001b[39m\n",
       "            ds\u001b[35m:bf16[2048,512]\u001b[39m dt\u001b[35m:bf16[8@data,1024,2048]\u001b[39m du\u001b[35m:bf16[512,50304]\u001b[39m dv\u001b[35m:bf16[8@data,1024,512]\u001b[39m = jit[\n",
       "            name=forward\n",
       "            ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; bb\u001b[35m:i32[8@data,1024]\u001b[39m a\u001b[35m:f32[50304,512]\u001b[39m b\u001b[35m:f32[512,8,64]\u001b[39m\n",
       "                c\u001b[35m:f32[512,8,64]\u001b[39m d\u001b[35m:f32[512,8,64]\u001b[39m e\u001b[35m:f32[8,64,512]\u001b[39m f\u001b[35m:f32[512,2048]\u001b[39m g\u001b[35m:f32[2048,512]\u001b[39m\n",
       "                h\u001b[35m:f32[512,50304]\u001b[39m z\u001b[35m:bf16[1,1024,1,32]\u001b[39m ba\u001b[35m:bf16[1,1024,1,32]\u001b[39m. \u001b[34;1mlet\n",
       "                \u001b[39;22mdw\u001b[35m:f32[50304,512]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                ] a\n",
       "                dx\u001b[35m:i32[8,1024]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                ] bb\n",
       "                dy\u001b[35m:bool[8,1024]\u001b[39m = lt dx 0:i32[]\n",
       "                dz\u001b[35m:i32[8,1024]\u001b[39m = add dx 50304:i32[]\n",
       "                ea\u001b[35m:i32[8,1024]\u001b[39m = select_n dy dx dz\n",
       "                ce\u001b[35m:i32[8,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1)\n",
       "                  shape=(8, 1024, 1)\n",
       "                  sharding=None\n",
       "                ] ea\n",
       "                eb\u001b[35m:f32[8,1024,512]\u001b[39m = gather[\n",
       "                  dimension_numbers=GatherDimensionNumbers(offset_dims=(2,), collapsed_slice_dims=(0,), start_index_map=(0,), operand_batching_dims=(), start_indices_batching_dims=())\n",
       "                  fill_value=None\n",
       "                  indices_are_sorted=False\n",
       "                  mode=GatherScatterMode.PROMISE_IN_BOUNDS\n",
       "                  slice_sizes=(1, 512)\n",
       "                  unique_indices=False\n",
       "                ] dw ce\n",
       "                ec\u001b[35m:f32[8@data,1024,512]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                ] eb\n",
       "                ed\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] ec\n",
       "                ee\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] ed\n",
       "                ef\u001b[35m:f32[8@data,1024,512]\u001b[39m = integer_pow[y=2] ee\n",
       "                eg\u001b[35m:f32[8@data,1024,512]\u001b[39m = integer_pow[y=1] ee\n",
       "                cf\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul 2.0:f32[] eg\n",
       "                eh\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[axes=(2,) out_sharding=None] ef\n",
       "                ei\u001b[35m:f32[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1)\n",
       "                  shape=(8, 1024, 1)\n",
       "                  sharding=None\n",
       "                ] eh\n",
       "                ej\u001b[35m:f32[8@data,1024,1]\u001b[39m = div ei 512.0:f32[]\n",
       "                ek\u001b[35m:f32[8@data,1024,1]\u001b[39m = add ej 9.999999974752427e-07:f32[]\n",
       "                ch\u001b[35m:f32[8@data,1024,1]\u001b[39m = rsqrt ek\n",
       "                el\u001b[35m:f32[8@data,1024,1]\u001b[39m = div ch ek\n",
       "                cg\u001b[35m:f32[8@data,1024,1]\u001b[39m = mul -0.5:f32[] el\n",
       "                ci\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] ed\n",
       "                em\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul ci ch\n",
       "                ck\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] em\n",
       "                cj\u001b[35m:bf16[512,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] b\n",
       "                en\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] ck cj\n",
       "                cl\u001b[35m:bf16[512,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] c\n",
       "                eo\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] ck cl\n",
       "                cm\u001b[35m:bf16[512,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] d\n",
       "                ep\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] ck cm\n",
       "                eq\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = slice[\n",
       "                  limit_indices=(8, 1024, 8, 32)\n",
       "                  start_indices=(0, 0, 0, 0)\n",
       "                  strides=None\n",
       "                ] en\n",
       "                er\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = slice[\n",
       "                  limit_indices=(8, 1024, 8, 64)\n",
       "                  start_indices=(0, 0, 0, 32)\n",
       "                  strides=None\n",
       "                ] en\n",
       "                es\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul eq z\n",
       "                et\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul er ba\n",
       "                eu\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add es et\n",
       "                cn\u001b[35m:bf16[1,1024,1,32]\u001b[39m = neg ba\n",
       "                ev\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul eq cn\n",
       "                ew\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul er z\n",
       "                ex\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add ev ew\n",
       "                ey\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = concatenate[dimension=3] eu ex\n",
       "                ez\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = slice[\n",
       "                  limit_indices=(8, 1024, 8, 32)\n",
       "                  start_indices=(0, 0, 0, 0)\n",
       "                  strides=None\n",
       "                ] eo\n",
       "                fa\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = slice[\n",
       "                  limit_indices=(8, 1024, 8, 64)\n",
       "                  start_indices=(0, 0, 0, 32)\n",
       "                  strides=None\n",
       "                ] eo\n",
       "                fb\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul ez z\n",
       "                fc\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul fa ba\n",
       "                fd\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add fb fc\n",
       "                co\u001b[35m:bf16[1,1024,1,32]\u001b[39m = neg ba\n",
       "                fe\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul ez co\n",
       "                ff\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul fa z\n",
       "                fg\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add fe ff\n",
       "                fh\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = concatenate[dimension=3] fd fg\n",
       "                fi\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] ey\n",
       "                fj\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = integer_pow[y=2] fi\n",
       "                fk\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = integer_pow[y=1] fi\n",
       "                cp\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul 2.0:f32[] fk\n",
       "                fl\u001b[35m:f32[8@data,1024,8]\u001b[39m = reduce_sum[axes=(3,) out_sharding=None] fj\n",
       "                fm\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1, 2)\n",
       "                  shape=(8, 1024, 8, 1)\n",
       "                  sharding=None\n",
       "                ] fl\n",
       "                fn\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = div fm 64.0:f32[]\n",
       "                fo\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = add fn 9.999999974752427e-07:f32[]\n",
       "                cr\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = rsqrt fo\n",
       "                fp\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = div cr fo\n",
       "                cq\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = mul -0.5:f32[] fp\n",
       "                cs\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] ey\n",
       "                fq\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul cs cr\n",
       "                cy\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] fq\n",
       "                fr\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] fh\n",
       "                fs\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = integer_pow[y=2] fr\n",
       "                ft\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = integer_pow[y=1] fr\n",
       "                ct\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul 2.0:f32[] ft\n",
       "                fu\u001b[35m:f32[8@data,1024,8]\u001b[39m = reduce_sum[axes=(3,) out_sharding=None] fs\n",
       "                fv\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1, 2)\n",
       "                  shape=(8, 1024, 8, 1)\n",
       "                  sharding=None\n",
       "                ] fu\n",
       "                fw\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = div fv 64.0:f32[]\n",
       "                fx\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = add fw 9.999999974752427e-07:f32[]\n",
       "                cv\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = rsqrt fx\n",
       "                fy\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = div cv fx\n",
       "                cu\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = mul -0.5:f32[] fy\n",
       "                cw\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] fh\n",
       "                fz\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul cw cv\n",
       "                ga\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] fz\n",
       "                gb\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None, None))\n",
       "                ] ga\n",
       "                gc\u001b[35m:bf16[8,1024,8,1,64]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1, 2, 4)\n",
       "                  shape=(8, 1024, 8, 1, 64)\n",
       "                  sharding=None\n",
       "                ] gb\n",
       "                gd\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 8, 64)\n",
       "                  sharding=None\n",
       "                ] gc\n",
       "                cx\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] gd\n",
       "                ge\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None, None))\n",
       "                ] ep\n",
       "                gf\u001b[35m:bf16[8,1024,8,1,64]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1, 2, 4)\n",
       "                  shape=(8, 1024, 8, 1, 64)\n",
       "                  sharding=None\n",
       "                ] ge\n",
       "                gg\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 8, 64)\n",
       "                  sharding=None\n",
       "                ] gf\n",
       "                dg\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] gg\n",
       "                gh\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([3], [3]), ([0, 2], [0, 2]))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] cy cx\n",
       "                cz\u001b[35m:bf16[]\u001b[39m = rsqrt 64.0:bf16[]\n",
       "                gi\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = mul gh cz\n",
       "                gj\u001b[35m:bf16[1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=()\n",
       "                  shape=(1024, 1024)\n",
       "                  sharding=None\n",
       "                ] 1.0:bf16[]\n",
       "                gk\u001b[35m:bf16[1024,1024]\u001b[39m = jit[\n",
       "                  name=tril\n",
       "                  ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; gj\u001b[35m:bf16[1024,1024]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mgl\u001b[35m:i32[1024,1024]\u001b[39m = iota[\n",
       "                        dimension=0\n",
       "                        dtype=int32\n",
       "                        shape=(1024, 1024)\n",
       "                        sharding=None\n",
       "                      ] \n",
       "                      gm\u001b[35m:i32[1024,1024]\u001b[39m = add gl 0:i32[]\n",
       "                      gn\u001b[35m:i32[1024,1024]\u001b[39m = iota[\n",
       "                        dimension=1\n",
       "                        dtype=int32\n",
       "                        shape=(1024, 1024)\n",
       "                        sharding=None\n",
       "                      ] \n",
       "                      go\u001b[35m:bool[1024,1024]\u001b[39m = ge gm gn\n",
       "                      gp\u001b[35m:bf16[1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                        broadcast_dimensions=()\n",
       "                        shape=(1024, 1024)\n",
       "                        sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                      ] 0.0:bf16[]\n",
       "                      gk\u001b[35m:bf16[1024,1024]\u001b[39m = select_n go gp gj\n",
       "                    \u001b[34;1min \u001b[39;22m(gk,) }\n",
       "                ] gj\n",
       "                gq\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m da\u001b[35m:bool[8@data,8,1024,1024]\u001b[39m db\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = jit[\n",
       "                  name=_where\n",
       "                  ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; gk\u001b[35m:bf16[1024,1024]\u001b[39m gi\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m\n",
       "                      gr\u001b[35m:bf16[]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mgs\u001b[35m:bf16[]\u001b[39m = reshard[\n",
       "                        concrete_mesh=None\n",
       "                        dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec())\n",
       "                      ] 0.0:bf16[]\n",
       "                      gt\u001b[35m:bool[1024,1024]\u001b[39m = ne gk gs\n",
       "                      da\u001b[35m:bool[8@data,8,1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                        broadcast_dimensions=(2, 3)\n",
       "                        shape=(8, 8, 1024, 1024)\n",
       "                        sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                      ] gt\n",
       "                      gu\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                        broadcast_dimensions=()\n",
       "                        shape=(8, 8, 1024, 1024)\n",
       "                        sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                      ] gr\n",
       "                      gq\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = select_n da gu gi\n",
       "                      db\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                        broadcast_dimensions=()\n",
       "                        shape=(8, 8, 1024, 1024)\n",
       "                        sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                      ] 0.0:bf16[]\n",
       "                    \u001b[34;1min \u001b[39;22m(gq, da, db) }\n",
       "                ] gk gi -inf:bf16[]\n",
       "                gv\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] gq\n",
       "                gw\u001b[35m:f32[8@data,8,1024]\u001b[39m = reduce_max[axes=(3,)] gv\n",
       "                gx\u001b[35m:f32[8@data,8,1024]\u001b[39m = max -inf:f32[] gw\n",
       "                gy\u001b[35m:f32[8@data,8,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1, 2)\n",
       "                  shape=(8, 8, 1024, 1)\n",
       "                  sharding=None\n",
       "                ] gx\n",
       "                gz\u001b[35m:f32[8@data,8,1024,1]\u001b[39m = stop_gradient gy\n",
       "                ha\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = sub gv gz\n",
       "                dc\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = exp ha\n",
       "                hb\u001b[35m:f32[8@data,8,1024]\u001b[39m = reduce_sum[axes=(3,) out_sharding=None] dc\n",
       "                dd\u001b[35m:f32[8@data,8,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1, 2)\n",
       "                  shape=(8, 8, 1024, 1)\n",
       "                  sharding=None\n",
       "                ] hb\n",
       "                hc\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = div dc dd\n",
       "                de\u001b[35m:f32[8@data,8,1024,1]\u001b[39m = integer_pow[y=-2] dd\n",
       "                df\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] hc\n",
       "                hd\u001b[35m:bf16[8@data,8,64,1024]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([1], [3]), ([0, 2], [0, 1]))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] dg df\n",
       "                di\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = transpose[permutation=(0, 3, 1, 2)] hd\n",
       "                dh\u001b[35m:bf16[8,64,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] e\n",
       "                he\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([3, 2], [1, 0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] di dh\n",
       "                hf\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add ed he\n",
       "                hg\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] hf\n",
       "                hh\u001b[35m:f32[8@data,1024,512]\u001b[39m = integer_pow[y=2] hg\n",
       "                hi\u001b[35m:f32[8@data,1024,512]\u001b[39m = integer_pow[y=1] hg\n",
       "                dj\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul 2.0:f32[] hi\n",
       "                hj\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[axes=(2,) out_sharding=None] hh\n",
       "                hk\u001b[35m:f32[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(0, 1)\n",
       "                  shape=(8, 1024, 1)\n",
       "                  sharding=None\n",
       "                ] hj\n",
       "                hl\u001b[35m:f32[8@data,1024,1]\u001b[39m = div hk 512.0:f32[]\n",
       "                hm\u001b[35m:f32[8@data,1024,1]\u001b[39m = add hl 9.999999974752427e-07:f32[]\n",
       "                dl\u001b[35m:f32[8@data,1024,1]\u001b[39m = rsqrt hm\n",
       "                hn\u001b[35m:f32[8@data,1024,1]\u001b[39m = div dl hm\n",
       "                dk\u001b[35m:f32[8@data,1024,1]\u001b[39m = mul -0.5:f32[] hn\n",
       "                dm\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  weak_type=False\n",
       "                ] hf\n",
       "                ho\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul dm dl\n",
       "                do\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] ho\n",
       "                dn\u001b[35m:bf16[512,2048]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] f\n",
       "                dr\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] do dn\n",
       "                dt\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = jit[\n",
       "                  name=silu\n",
       "                  ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; dr\u001b[35m:bf16[8@data,1024,2048]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mdq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = logistic dr\n",
       "                      hp\u001b[35m:bf16[]\u001b[39m = reshard[\n",
       "                        concrete_mesh=None\n",
       "                        dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec())\n",
       "                      ] 1.0:bf16[]\n",
       "                      hq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = sub hp dq\n",
       "                      dp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = mul dq hq\n",
       "                      dt\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = mul dr dq\n",
       "                    \u001b[34;1min \u001b[39;22m(dt, dp, dq) }\n",
       "                ] dr\n",
       "                ds\u001b[35m:bf16[2048,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] g\n",
       "                hr\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] dt ds\n",
       "                dv\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add hf hr\n",
       "                du\u001b[35m:bf16[512,50304]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  weak_type=False\n",
       "                ] h\n",
       "                cd\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [0]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] dv du\n",
       "              \u001b[34;1min \u001b[39;22m(cd, ce, cf, cg, ch, ci, cj, ck, cl, cm, cn, co, cp, cq, cr, cs,\n",
       "                ct, cu, cv, cw, cx, cy, cz, da, db, dc, dd, de, df, dg, dh, di, dj,\n",
       "                dk, dl, dm, dn, do, dp, dq, dr, ds, dt, du, dv) }\n",
       "          ] bb a b c d e f g h z ba\n",
       "          hs\u001b[35m:i32[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=(0, 1)\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=None\n",
       "          ] bc\n",
       "          ht\u001b[35m:bf16[8@data,1024,1]\u001b[39m hu\u001b[35m:i32[8@data,1024,1,1]\u001b[39m = jit[\n",
       "            name=take_along_axis\n",
       "            ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; cd\u001b[35m:bf16[8@data,1024,50304]\u001b[39m hs\u001b[35m:i32[8@data,1024,1]\u001b[39m. \u001b[34;1mlet\n",
       "                \u001b[39;22mhv\u001b[35m:bool[8@data,1024,1]\u001b[39m = lt hs 0:i32[]\n",
       "                hw\u001b[35m:i32[8@data,1024,1]\u001b[39m = add hs 50304:i32[]\n",
       "                hx\u001b[35m:i32[8@data,1024,1]\u001b[39m = select_n hv hs hw\n",
       "                hu\u001b[35m:i32[8@data,1024,1,1]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 1, 1)\n",
       "                  sharding=None\n",
       "                ] hx\n",
       "                ht\u001b[35m:bf16[8@data,1024,1]\u001b[39m = gather[\n",
       "                  dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(2,), start_index_map=(2,), operand_batching_dims=(0, 1), start_indices_batching_dims=(0, 1))\n",
       "                  fill_value=nan\n",
       "                  indices_are_sorted=False\n",
       "                  mode=GatherScatterMode.FILL_OR_DROP\n",
       "                  slice_sizes=(1, 1, 1)\n",
       "                  unique_indices=False\n",
       "                ] cd hu\n",
       "              \u001b[34;1min \u001b[39;22m(ht, hu) }\n",
       "          ] cd hs\n",
       "          hy\u001b[35m:bf16[8@data,1024]\u001b[39m = reduce_max[axes=(2,)] cd\n",
       "          hz\u001b[35m:bf16[8@data,1024,1]\u001b[39m = reshape[\n",
       "            dimensions=None\n",
       "            new_sizes=(8, 1024, 1)\n",
       "            sharding=None\n",
       "          ] hy\n",
       "          ia\u001b[35m:bool[8@data,1024,50304]\u001b[39m = eq cd hz\n",
       "          ib\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = convert_element_type[\n",
       "            new_dtype=bfloat16\n",
       "            weak_type=False\n",
       "          ] ia\n",
       "          _\u001b[35m:bf16[8@data,1024]\u001b[39m = reduce_sum[axes=(2,) out_sharding=None] ib\n",
       "          ic\u001b[35m:bf16[8@data,1024]\u001b[39m = max -inf:bf16[] hy\n",
       "          id\u001b[35m:bool[8@data,1024]\u001b[39m = eq hy ic\n",
       "          ie\u001b[35m:bf16[8@data,1024]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] 1.0:bf16[]\n",
       "          if\u001b[35m:bf16[8@data,1024]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] 0.0:bf16[]\n",
       "          ig\u001b[35m:bf16[8@data,1024]\u001b[39m = select_n id if ie\n",
       "          ih\u001b[35m:bool[8@data,1024]\u001b[39m = eq -inf:bf16[] ic\n",
       "          ii\u001b[35m:bf16[8@data,1024]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] 2.0:bf16[]\n",
       "          ij\u001b[35m:bf16[8@data,1024]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] 1.0:bf16[]\n",
       "          ik\u001b[35m:bf16[8@data,1024]\u001b[39m = select_n ih ij ii\n",
       "          _\u001b[35m:bf16[8@data,1024]\u001b[39m = div ig ik\n",
       "          il\u001b[35m:bf16[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=(0, 1)\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=None\n",
       "          ] ic\n",
       "          im\u001b[35m:bool[8@data,1024,1]\u001b[39m = is_finite il\n",
       "          in\u001b[35m:bf16[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] 0.0:bf16[]\n",
       "          io\u001b[35m:bf16[8@data,1024,1]\u001b[39m = select_n im in il\n",
       "          _\u001b[35m:bf16[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] 0.0:bf16[]\n",
       "          ip\u001b[35m:bf16[8@data,1024,1]\u001b[39m = stop_gradient io\n",
       "          iq\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = sub cd ip\n",
       "          ir\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = exp iq\n",
       "          is\u001b[35m:f32[8@data,1024,50304]\u001b[39m = convert_element_type[\n",
       "            new_dtype=float32\n",
       "            weak_type=False\n",
       "          ] ir\n",
       "          it\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[axes=(2,) out_sharding=None] is\n",
       "          iu\u001b[35m:f32[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=(0, 1)\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=None\n",
       "          ] it\n",
       "          iv\u001b[35m:bf16[8@data,1024,1]\u001b[39m = convert_element_type[\n",
       "            new_dtype=bfloat16\n",
       "            weak_type=False\n",
       "          ] iu\n",
       "          _\u001b[35m:bf16[8@data,1024,1]\u001b[39m = sign iv\n",
       "          iw\u001b[35m:bf16[8@data,1024,1]\u001b[39m = abs iv\n",
       "          ix\u001b[35m:bf16[]\u001b[39m = reshard[\n",
       "            concrete_mesh=None\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec())\n",
       "          ] 0.0:bf16[]\n",
       "          iy\u001b[35m:bool[8@data,1024,1]\u001b[39m = ge iv ix\n",
       "          iz\u001b[35m:bf16[8@data,1024,1]\u001b[39m = log iw\n",
       "          ja\u001b[35m:bf16[8@data,1024,1]\u001b[39m = add iz ip\n",
       "          jb\u001b[35m:bf16[8@data,1024,1]\u001b[39m = sub ja ht\n",
       "          jc\u001b[35m:f32[8@data,1024,1]\u001b[39m = convert_element_type[\n",
       "            new_dtype=float32\n",
       "            weak_type=False\n",
       "          ] jb\n",
       "          jd\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0, 1, 2) out_sharding=None] jc\n",
       "          je\u001b[35m:f32[]\u001b[39m = div jd 8192.0:f32[]\n",
       "          cc\u001b[35m:bf16[]\u001b[39m = convert_element_type[new_dtype=bfloat16 weak_type=False] je\n",
       "          jf\u001b[35m:bf16[]\u001b[39m = reshard[\n",
       "            concrete_mesh=None\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec())\n",
       "          ] 1.0:bf16[]\n",
       "          jg\u001b[35m:f32[]\u001b[39m = convert_element_type[\n",
       "            new_dtype=float32\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec())\n",
       "            weak_type=False\n",
       "          ] jf\n",
       "          jh\u001b[35m:f32[]\u001b[39m = div jg 8192.0:f32[]\n",
       "          ji\u001b[35m:f32[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] jh\n",
       "          jj\u001b[35m:bf16[8@data,1024,1]\u001b[39m = convert_element_type[\n",
       "            new_dtype=bfloat16\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "            weak_type=False\n",
       "          ] ji\n",
       "          jk\u001b[35m:bf16[8@data,1024,1]\u001b[39m = neg jj\n",
       "          jl\u001b[35m:bf16[8@data,1024,1]\u001b[39m = div jj iw\n",
       "          jm\u001b[35m:bf16[8@data,1024,1]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=()\n",
       "            shape=(8, 1024, 1)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] 0.0:bf16[]\n",
       "          jn\u001b[35m:bf16[8@data,1024,1]\u001b[39m = select_n iy jl jm\n",
       "          jo\u001b[35m:bf16[8@data,1024,1]\u001b[39m = select_n iy jm jl\n",
       "          jp\u001b[35m:bf16[8@data,1024,1]\u001b[39m = neg jn\n",
       "          jq\u001b[35m:bf16[8@data,1024,1]\u001b[39m = add_any jo jp\n",
       "          jr\u001b[35m:f32[8@data,1024,1]\u001b[39m = convert_element_type[\n",
       "            new_dtype=float32\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "            weak_type=False\n",
       "          ] jq\n",
       "          js\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[\n",
       "            axes=(np.int64(2),)\n",
       "            out_sharding=None\n",
       "          ] jr\n",
       "          jt\u001b[35m:f32[8@data,1024,50304]\u001b[39m = broadcast_in_dim[\n",
       "            broadcast_dimensions=(np.int64(0), np.int64(1))\n",
       "            shape=(8, 1024, 50304)\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] js\n",
       "          ju\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = convert_element_type[\n",
       "            new_dtype=bfloat16\n",
       "            sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "            weak_type=False\n",
       "          ] jt\n",
       "          jv\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = mul ju ir\n",
       "          jw\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = jit[\n",
       "            name=take_along_axis\n",
       "            ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; hu\u001b[35m:i32[8@data,1024,1,1]\u001b[39m jk\u001b[35m:bf16[8@data,1024,1]\u001b[39m. \u001b[34;1mlet\n",
       "                \u001b[39;22mjx\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=()\n",
       "                  shape=(8, 1024, 50304)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                ] 0.0:bf16[]\n",
       "                jw\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = scatter-add[\n",
       "                  dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(2,), scatter_dims_to_operand_dims=(2,), operand_batching_dims=(0, 1), scatter_indices_batching_dims=(0, 1))\n",
       "                  indices_are_sorted=False\n",
       "                  mode=GatherScatterMode.FILL_OR_DROP\n",
       "                  unique_indices=False\n",
       "                  update_consts=()\n",
       "                  update_jaxpr={ \u001b[34;1mlambda \u001b[39;22m; jy\u001b[35m:bf16[]\u001b[39m jz\u001b[35m:bf16[]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mka\u001b[35m:bf16[]\u001b[39m = add jy jz\n",
       "                    \u001b[34;1min \u001b[39;22m(ka,) }\n",
       "                ] jx hu jk\n",
       "              \u001b[34;1min \u001b[39;22m(jw,) }\n",
       "          ] hu jk\n",
       "          kb\u001b[35m:bf16[8@data,1024,50304]\u001b[39m = add_any jv jw\n",
       "          kc\u001b[35m:f32[50304,512]\u001b[39m kd\u001b[35m:f32[512,8,64]\u001b[39m ke\u001b[35m:f32[512,8,64]\u001b[39m kf\u001b[35m:f32[512,8,64]\u001b[39m kg\u001b[35m:f32[8,64,512]\u001b[39m\n",
       "            kh\u001b[35m:f32[512,2048]\u001b[39m ki\u001b[35m:f32[2048,512]\u001b[39m kj\u001b[35m:f32[512,50304]\u001b[39m = jit[\n",
       "            name=forward\n",
       "            ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; ce\u001b[35m:i32[8,1024,1]\u001b[39m cf\u001b[35m:f32[8@data,1024,512]\u001b[39m cg\u001b[35m:f32[8@data,1024,1]\u001b[39m\n",
       "                ch\u001b[35m:f32[8@data,1024,1]\u001b[39m ci\u001b[35m:f32[8@data,1024,512]\u001b[39m cj\u001b[35m:bf16[512,8,64]\u001b[39m ck\u001b[35m:bf16[8@data,1024,512]\u001b[39m\n",
       "                cl\u001b[35m:bf16[512,8,64]\u001b[39m cm\u001b[35m:bf16[512,8,64]\u001b[39m z\u001b[35m:bf16[1,1024,1,32]\u001b[39m ba\u001b[35m:bf16[1,1024,1,32]\u001b[39m\n",
       "                cn\u001b[35m:bf16[1,1024,1,32]\u001b[39m co\u001b[35m:bf16[1,1024,1,32]\u001b[39m cp\u001b[35m:f32[8@data,1024,8,64]\u001b[39m\n",
       "                cq\u001b[35m:f32[8@data,1024,8,1]\u001b[39m cr\u001b[35m:f32[8@data,1024,8,1]\u001b[39m cs\u001b[35m:f32[8@data,1024,8,64]\u001b[39m\n",
       "                ct\u001b[35m:f32[8@data,1024,8,64]\u001b[39m cu\u001b[35m:f32[8@data,1024,8,1]\u001b[39m cv\u001b[35m:f32[8@data,1024,8,1]\u001b[39m\n",
       "                cw\u001b[35m:f32[8@data,1024,8,64]\u001b[39m cx\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m cy\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m\n",
       "                cz\u001b[35m:bf16[]\u001b[39m da\u001b[35m:bool[8@data,8,1024,1024]\u001b[39m db\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m\n",
       "                dc\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m dd\u001b[35m:f32[8@data,8,1024,1]\u001b[39m de\u001b[35m:f32[8@data,8,1024,1]\u001b[39m\n",
       "                df\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m dg\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m dh\u001b[35m:bf16[8,64,512]\u001b[39m\n",
       "                di\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m dj\u001b[35m:f32[8@data,1024,512]\u001b[39m dk\u001b[35m:f32[8@data,1024,1]\u001b[39m\n",
       "                dl\u001b[35m:f32[8@data,1024,1]\u001b[39m dm\u001b[35m:f32[8@data,1024,512]\u001b[39m dn\u001b[35m:bf16[512,2048]\u001b[39m do\u001b[35m:bf16[8@data,1024,512]\u001b[39m\n",
       "                dp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dr\u001b[35m:bf16[8@data,1024,2048]\u001b[39m\n",
       "                ds\u001b[35m:bf16[2048,512]\u001b[39m dt\u001b[35m:bf16[8@data,1024,2048]\u001b[39m du\u001b[35m:bf16[512,50304]\u001b[39m dv\u001b[35m:bf16[8@data,1024,512]\u001b[39m\n",
       "                kb\u001b[35m:bf16[8@data,1024,50304]\u001b[39m. \u001b[34;1mlet\n",
       "                \u001b[39;22mkk\u001b[35m:bf16[50304,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] kb dv\n",
       "                kl\u001b[35m:bf16[512,50304]\u001b[39m = transpose[permutation=(1, 0)] kk\n",
       "                km\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] kb du\n",
       "                kj\u001b[35m:f32[512,50304]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                  weak_type=False\n",
       "                ] kl\n",
       "                kn\u001b[35m:bf16[512,2048]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] km dt\n",
       "                ko\u001b[35m:bf16[2048,512]\u001b[39m = transpose[permutation=(1, 0)] kn\n",
       "                kp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] km ds\n",
       "                ki\u001b[35m:f32[2048,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                  weak_type=False\n",
       "                ] ko\n",
       "                kq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = jit[\n",
       "                  name=silu\n",
       "                  ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; dp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m dq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m\n",
       "                      dr\u001b[35m:bf16[8@data,1024,2048]\u001b[39m kp\u001b[35m:bf16[8@data,1024,2048]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mkr\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = mul dr kp\n",
       "                      ks\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = mul kp dq\n",
       "                      kt\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = mul kr dp\n",
       "                      kq\u001b[35m:bf16[8@data,1024,2048]\u001b[39m = add_any ks kt\n",
       "                    \u001b[34;1min \u001b[39;22m(kq,) }\n",
       "                ] dp dq dr kp\n",
       "                ku\u001b[35m:bf16[2048,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] kq do\n",
       "                kv\u001b[35m:bf16[512,2048]\u001b[39m = transpose[permutation=(1, 0)] ku\n",
       "                kw\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] kq dn\n",
       "                kh\u001b[35m:f32[512,2048]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                  weak_type=False\n",
       "                ] kv\n",
       "                kx\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] kw\n",
       "                ky\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul dm kx\n",
       "                kz\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[axes=(2,) out_sharding=None] ky\n",
       "                la\u001b[35m:f32[8@data,1024,1]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 1)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                ] kz\n",
       "                lb\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul kx dl\n",
       "                lc\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] lb\n",
       "                ld\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add_any km lc\n",
       "                le\u001b[35m:f32[8@data,1024,1]\u001b[39m = mul la dk\n",
       "                lf\u001b[35m:f32[8@data,1024,1]\u001b[39m = div le 512.0:f32[]\n",
       "                lg\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(2),)\n",
       "                  out_sharding=None\n",
       "                ] lf\n",
       "                lh\u001b[35m:f32[8@data,1024,512]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(np.int64(0), np.int64(1))\n",
       "                  shape=(8, 1024, 512)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                ] lg\n",
       "                li\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul lh dj\n",
       "                lj\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] li\n",
       "                lk\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add_any ld lj\n",
       "                ll\u001b[35m:bf16[512,8,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] lk di\n",
       "                lm\u001b[35m:bf16[8,64,512]\u001b[39m = transpose[permutation=(1, 2, 0)] ll\n",
       "                ln\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [2]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] lk dh\n",
       "                kg\u001b[35m:f32[8,64,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  weak_type=False\n",
       "                ] lm\n",
       "                lo\u001b[35m:bf16[8@data,8,64,1024]\u001b[39m = transpose[permutation=(0, 2, 3, 1)] ln\n",
       "                lp\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [3]), ([0, 1], [0, 2]))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] lo dg\n",
       "                lq\u001b[35m:bf16[8@data,8,64,1024]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([3], [2]), ([0, 1], [0, 1]))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] lo df\n",
       "                lr\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = transpose[permutation=(0, 3, 1, 2)] lq\n",
       "                ls\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] lp\n",
       "                lt\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = mul ls de\n",
       "                lu\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = mul lt dc\n",
       "                lv\u001b[35m:f32[8@data,8,1024]\u001b[39m = reduce_sum[axes=(3,) out_sharding=None] lu\n",
       "                lw\u001b[35m:f32[8@data,8,1024,1]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 8, 1024, 1)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] lv\n",
       "                lx\u001b[35m:f32[8@data,8,1024,1]\u001b[39m = neg lw\n",
       "                ly\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = div ls dd\n",
       "                lz\u001b[35m:f32[8@data,8,1024]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(3),)\n",
       "                  out_sharding=None\n",
       "                ] lx\n",
       "                ma\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(np.int64(0), np.int64(1), np.int64(2))\n",
       "                  shape=(8, 8, 1024, 1024)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] lz\n",
       "                mb\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = add_any ly ma\n",
       "                mc\u001b[35m:f32[8@data,8,1024,1024]\u001b[39m = mul mb dc\n",
       "                md\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] mc\n",
       "                me\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = jit[\n",
       "                  name=_where\n",
       "                  ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; da\u001b[35m:bool[8@data,8,1024,1024]\u001b[39m db\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m\n",
       "                      md\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mmf\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = broadcast_in_dim[\n",
       "                        broadcast_dimensions=()\n",
       "                        shape=(8, 8, 1024, 1024)\n",
       "                        sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                      ] 0.0:bf16[]\n",
       "                      me\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = select_n da mf md\n",
       "                    \u001b[34;1min \u001b[39;22m(me,) }\n",
       "                ] da db md\n",
       "                mg\u001b[35m:bf16[8@data,8,1024,1024]\u001b[39m = mul me cz\n",
       "                mh\u001b[35m:bf16[8@data,8,1024,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2], [1]), ([0, 1], [0, 2]))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] mg cy\n",
       "                mi\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = transpose[permutation=(0, 2, 1, 3)] mh\n",
       "                mj\u001b[35m:bf16[8@data,8,1024,64]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([3], [1]), ([0, 1], [0, 2]))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] mg cx\n",
       "                mk\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = transpose[permutation=(0, 2, 1, 3)] mj\n",
       "                ml\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None, None))\n",
       "                ] lr\n",
       "                mm\u001b[35m:bf16[8,1024,8,1,64]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 8, 1, 64)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None, None, None))\n",
       "                ] ml\n",
       "                mn\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(3),)\n",
       "                  out_sharding=None\n",
       "                ] mm\n",
       "                mo\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] mn\n",
       "                mp\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None, None))\n",
       "                ] mi\n",
       "                mq\u001b[35m:bf16[8,1024,8,1,64]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 8, 1, 64)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None, None, None))\n",
       "                ] mp\n",
       "                mr\u001b[35m:bf16[8,1024,8,64]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(3),)\n",
       "                  out_sharding=None\n",
       "                ] mq\n",
       "                ms\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] mr\n",
       "                mt\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] ms\n",
       "                mu\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul cw mt\n",
       "                mv\u001b[35m:f32[8@data,1024,8]\u001b[39m = reduce_sum[axes=(3,) out_sharding=None] mu\n",
       "                mw\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 8, 1)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] mv\n",
       "                mx\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul mt cv\n",
       "                my\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] mx\n",
       "                mz\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = mul mw cu\n",
       "                na\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = div mz 64.0:f32[]\n",
       "                nb\u001b[35m:f32[8@data,1024,8]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(3),)\n",
       "                  out_sharding=None\n",
       "                ] na\n",
       "                nc\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(np.int64(0), np.int64(1), np.int64(2))\n",
       "                  shape=(8, 1024, 8, 64)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] nb\n",
       "                nd\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul nc ct\n",
       "                ne\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] nd\n",
       "                nf\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = add_any my ne\n",
       "                ng\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] mk\n",
       "                nh\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul cs ng\n",
       "                ni\u001b[35m:f32[8@data,1024,8]\u001b[39m = reduce_sum[axes=(3,) out_sharding=None] nh\n",
       "                nj\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 8, 1)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] ni\n",
       "                nk\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul ng cr\n",
       "                nl\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] nk\n",
       "                nm\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = mul nj cq\n",
       "                nn\u001b[35m:f32[8@data,1024,8,1]\u001b[39m = div nm 64.0:f32[]\n",
       "                no\u001b[35m:f32[8@data,1024,8]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(3),)\n",
       "                  out_sharding=None\n",
       "                ] nn\n",
       "                np\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(np.int64(0), np.int64(1), np.int64(2))\n",
       "                  shape=(8, 1024, 8, 64)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                ] no\n",
       "                nq\u001b[35m:f32[8@data,1024,8,64]\u001b[39m = mul np cp\n",
       "                nr\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None, None))\n",
       "                  weak_type=False\n",
       "                ] nq\n",
       "                ns\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = add_any nl nr\n",
       "                nt\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m nu\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = split[\n",
       "                  axis=3\n",
       "                  sizes=(32, 32)\n",
       "                ] nf\n",
       "                nv\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul nu z\n",
       "                nw\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul nu co\n",
       "                nx\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul nt ba\n",
       "                ny\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add_any nv nx\n",
       "                nz\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul nt z\n",
       "                oa\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add_any nw nz\n",
       "                ob\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = pad[\n",
       "                  padding_config=((0, np.int64(0), 0), (0, np.int64(0), 0), (0, np.int64(0), 0), (32, np.int64(0), 0))\n",
       "                ] ny 0.0:bf16[]\n",
       "                oc\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = pad[\n",
       "                  padding_config=((0, np.int64(0), 0), (0, np.int64(0), 0), (0, np.int64(0), 0), (0, np.int64(32), 0))\n",
       "                ] oa 0.0:bf16[]\n",
       "                od\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = add_any ob oc\n",
       "                oe\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m of\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = split[\n",
       "                  axis=3\n",
       "                  sizes=(32, 32)\n",
       "                ] ns\n",
       "                og\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul of z\n",
       "                oh\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul of cn\n",
       "                oi\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul oe ba\n",
       "                oj\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add_any og oi\n",
       "                ok\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = mul oe z\n",
       "                ol\u001b[35m:bf16[8@data,1024,8,32]\u001b[39m = add_any oh ok\n",
       "                om\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = pad[\n",
       "                  padding_config=((0, np.int64(0), 0), (0, np.int64(0), 0), (0, np.int64(0), 0), (32, np.int64(0), 0))\n",
       "                ] oj 0.0:bf16[]\n",
       "                on\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = pad[\n",
       "                  padding_config=((0, np.int64(0), 0), (0, np.int64(0), 0), (0, np.int64(0), 0), (0, np.int64(32), 0))\n",
       "                ] ol 0.0:bf16[]\n",
       "                oo\u001b[35m:bf16[8@data,1024,8,64]\u001b[39m = add_any om on\n",
       "                op\u001b[35m:bf16[8,64,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] mo ck\n",
       "                oq\u001b[35m:bf16[512,8,64]\u001b[39m = transpose[permutation=(2, 0, 1)] op\n",
       "                or\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2, 3], [1, 2]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] mo cm\n",
       "                kf\u001b[35m:f32[512,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  weak_type=False\n",
       "                ] oq\n",
       "                os\u001b[35m:bf16[8,64,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] od ck\n",
       "                ot\u001b[35m:bf16[512,8,64]\u001b[39m = transpose[permutation=(2, 0, 1)] os\n",
       "                ou\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2, 3], [1, 2]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] od cl\n",
       "                ov\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add_any or ou\n",
       "                ke\u001b[35m:f32[512,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  weak_type=False\n",
       "                ] ot\n",
       "                ow\u001b[35m:bf16[8,64,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([0, 1], [0, 1]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] oo ck\n",
       "                ox\u001b[35m:bf16[512,8,64]\u001b[39m = transpose[permutation=(2, 0, 1)] ow\n",
       "                oy\u001b[35m:bf16[8@data,1024,512]\u001b[39m = dot_general[\n",
       "                  dimension_numbers=(([2, 3], [1, 2]), ([], []))\n",
       "                  out_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  preferred_element_type=bfloat16\n",
       "                ] oo cj\n",
       "                oz\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add_any ov oy\n",
       "                kd\u001b[35m:f32[512,8,64]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                  weak_type=False\n",
       "                ] ox\n",
       "                pa\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] oz\n",
       "                pb\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul ci pa\n",
       "                pc\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[axes=(2,) out_sharding=None] pb\n",
       "                pd\u001b[35m:f32[8@data,1024,1]\u001b[39m = reshape[\n",
       "                  dimensions=None\n",
       "                  new_sizes=(8, 1024, 1)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                ] pc\n",
       "                pe\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul pa ch\n",
       "                pf\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] pe\n",
       "                pg\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add_any lk pf\n",
       "                ph\u001b[35m:f32[8@data,1024,1]\u001b[39m = mul pd cg\n",
       "                pi\u001b[35m:f32[8@data,1024,1]\u001b[39m = div ph 512.0:f32[]\n",
       "                pj\u001b[35m:f32[8@data,1024]\u001b[39m = reduce_sum[\n",
       "                  axes=(np.int64(2),)\n",
       "                  out_sharding=None\n",
       "                ] pi\n",
       "                pk\u001b[35m:f32[8@data,1024,512]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=(np.int64(0), np.int64(1))\n",
       "                  shape=(8, 1024, 512)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                ] pj\n",
       "                pl\u001b[35m:f32[8@data,1024,512]\u001b[39m = mul pk cf\n",
       "                pm\u001b[35m:bf16[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=bfloat16\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] pl\n",
       "                pn\u001b[35m:bf16[8@data,1024,512]\u001b[39m = add_any pg pm\n",
       "                po\u001b[35m:f32[8@data,1024,512]\u001b[39m = convert_element_type[\n",
       "                  new_dtype=float32\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "                  weak_type=False\n",
       "                ] pn\n",
       "                pp\u001b[35m:f32[8,1024,512]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "                ] po\n",
       "                pq\u001b[35m:f32[50304,512]\u001b[39m = broadcast_in_dim[\n",
       "                  broadcast_dimensions=()\n",
       "                  shape=(50304, 512)\n",
       "                  sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Auto,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                ] 0.0:f32[]\n",
       "                pr\u001b[35m:f32[50304,512]\u001b[39m = scatter-add[\n",
       "                  dimension_numbers=ScatterDimensionNumbers(update_window_dims=(2,), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,), operand_batching_dims=(), scatter_indices_batching_dims=())\n",
       "                  indices_are_sorted=False\n",
       "                  mode=GatherScatterMode.PROMISE_IN_BOUNDS\n",
       "                  unique_indices=False\n",
       "                  update_consts=()\n",
       "                  update_jaxpr={ \u001b[34;1mlambda \u001b[39;22m; ps\u001b[35m:f32[]\u001b[39m pt\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mpu\u001b[35m:f32[]\u001b[39m = add ps pt\n",
       "                    \u001b[34;1min \u001b[39;22m(pu,) }\n",
       "                ] pq ce pp\n",
       "                kc\u001b[35m:f32[50304,512]\u001b[39m = reshard[\n",
       "                  concrete_mesh=None\n",
       "                  dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "                ] pr\n",
       "              \u001b[34;1min \u001b[39;22m(kc, kd, ke, kf, kg, kh, ki, kj) }\n",
       "          ] ce cf cg ch ci cj ck cl cm z ba cn co cp cq cr cs ct cu cv cw cx cy cz\n",
       "            da db dc dd de df dg dh di dj dk dl dm dn do dp dq dr ds dt du dv kb\n",
       "          pv\u001b[35m:f32[50304,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] kc\n",
       "          pw\u001b[35m:f32[512,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] kd\n",
       "          px\u001b[35m:f32[512,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] ke\n",
       "          py\u001b[35m:f32[512,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] kf\n",
       "          pz\u001b[35m:f32[8,64,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] kg\n",
       "          qa\u001b[35m:f32[512,2048]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] kh\n",
       "          qb\u001b[35m:f32[2048,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] ki\n",
       "          qc\u001b[35m:f32[512,50304]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] kj\n",
       "          qd\u001b[35m:f32[50304@data,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] j\n",
       "          qe\u001b[35m:f32[512@data,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] k\n",
       "          qf\u001b[35m:f32[512@data,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] l\n",
       "          qg\u001b[35m:f32[512@data,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] m\n",
       "          qh\u001b[35m:f32[8@data,64,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] n\n",
       "          qi\u001b[35m:f32[512@data,2048]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] o\n",
       "          qj\u001b[35m:f32[2048@data,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] p\n",
       "          qk\u001b[35m:f32[512@data,50304]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] q\n",
       "          ql\u001b[35m:f32[50304@data,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] r\n",
       "          qm\u001b[35m:f32[512@data,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] s\n",
       "          qn\u001b[35m:f32[512@data,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] t\n",
       "          qo\u001b[35m:f32[512@data,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] u\n",
       "          qp\u001b[35m:f32[8@data,64,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None, None))\n",
       "          ] v\n",
       "          qq\u001b[35m:f32[512@data,2048]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] w\n",
       "          qr\u001b[35m:f32[2048@data,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] x\n",
       "          qs\u001b[35m:f32[512@data,50304]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec('data', None))\n",
       "          ] y\n",
       "          qt\u001b[35m:f32[50304,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] a\n",
       "          qu\u001b[35m:f32[512,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] b\n",
       "          qv\u001b[35m:f32[512,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] c\n",
       "          qw\u001b[35m:f32[512,8,64]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] d\n",
       "          qx\u001b[35m:f32[8,64,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None, None))\n",
       "          ] e\n",
       "          qy\u001b[35m:f32[512,2048]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] f\n",
       "          qz\u001b[35m:f32[2048,512]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] g\n",
       "          ra\u001b[35m:f32[512,50304]\u001b[39m = reshard[\n",
       "            concrete_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            dst_sharding=NamedSharding(mesh=AbstractMesh('data': 8, axis_types=(Explicit,), device_kind=cpu, num_cores=None), spec=PartitionSpec(None, None))\n",
       "          ] h\n",
       "          rb\u001b[35m:f32[50304,512]\u001b[39m rc\u001b[35m:f32[512,8,64]\u001b[39m rd\u001b[35m:f32[512,8,64]\u001b[39m re\u001b[35m:f32[512,8,64]\u001b[39m rf\u001b[35m:f32[8,64,512]\u001b[39m\n",
       "            rg\u001b[35m:f32[512,2048]\u001b[39m rh\u001b[35m:f32[2048,512]\u001b[39m ri\u001b[35m:f32[512,50304]\u001b[39m bl\u001b[35m:i32[]\u001b[39m bm\u001b[35m:f32[50304@data,512]\u001b[39m\n",
       "            bn\u001b[35m:f32[512@data,8,64]\u001b[39m bo\u001b[35m:f32[512@data,8,64]\u001b[39m bp\u001b[35m:f32[512@data,8,64]\u001b[39m bq\u001b[35m:f32[8@data,64,512]\u001b[39m\n",
       "            br\u001b[35m:f32[512@data,2048]\u001b[39m bs\u001b[35m:f32[2048@data,512]\u001b[39m bt\u001b[35m:f32[512@data,50304]\u001b[39m bu\u001b[35m:f32[50304@data,512]\u001b[39m\n",
       "            bv\u001b[35m:f32[512@data,8,64]\u001b[39m bw\u001b[35m:f32[512@data,8,64]\u001b[39m bx\u001b[35m:f32[512@data,8,64]\u001b[39m by\u001b[35m:f32[8@data,64,512]\u001b[39m\n",
       "            bz\u001b[35m:f32[512@data,2048]\u001b[39m ca\u001b[35m:f32[2048@data,512]\u001b[39m cb\u001b[35m:f32[512@data,50304]\u001b[39m = shard_map[\n",
       "            check_vma=False\n",
       "            in_specs=(PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec())\n",
       "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; rj\u001b[35m:f32[50304,512]\u001b[39m rk\u001b[35m:f32[512,8,64]\u001b[39m rl\u001b[35m:f32[512,8,64]\u001b[39m\n",
       "                rm\u001b[35m:f32[512,8,64]\u001b[39m rn\u001b[35m:f32[8,64,512]\u001b[39m ro\u001b[35m:f32[512,2048]\u001b[39m rp\u001b[35m:f32[2048,512]\u001b[39m\n",
       "                rq\u001b[35m:f32[512,50304]\u001b[39m rr\u001b[35m:i32[]\u001b[39m rs\u001b[35m:f32[6288,512]\u001b[39m rt\u001b[35m:f32[64,8,64]\u001b[39m ru\u001b[35m:f32[64,8,64]\u001b[39m\n",
       "                rv\u001b[35m:f32[64,8,64]\u001b[39m rw\u001b[35m:f32[1,64,512]\u001b[39m rx\u001b[35m:f32[64,2048]\u001b[39m ry\u001b[35m:f32[256,512]\u001b[39m\n",
       "                rz\u001b[35m:f32[64,50304]\u001b[39m sa\u001b[35m:f32[6288,512]\u001b[39m sb\u001b[35m:f32[64,8,64]\u001b[39m sc\u001b[35m:f32[64,8,64]\u001b[39m\n",
       "                sd\u001b[35m:f32[64,8,64]\u001b[39m se\u001b[35m:f32[1,64,512]\u001b[39m sf\u001b[35m:f32[64,2048]\u001b[39m sg\u001b[35m:f32[256,512]\u001b[39m\n",
       "                sh\u001b[35m:f32[64,50304]\u001b[39m si\u001b[35m:f32[50304,512]\u001b[39m sj\u001b[35m:f32[512,8,64]\u001b[39m sk\u001b[35m:f32[512,8,64]\u001b[39m\n",
       "                sl\u001b[35m:f32[512,8,64]\u001b[39m sm\u001b[35m:f32[8,64,512]\u001b[39m sn\u001b[35m:f32[512,2048]\u001b[39m so\u001b[35m:f32[2048,512]\u001b[39m\n",
       "                sp\u001b[35m:f32[512,50304]\u001b[39m. \u001b[34;1mlet\n",
       "                \u001b[39;22msq\u001b[35m:i32[]\u001b[39m = axis_index[axis_name=data] \n",
       "                sr\u001b[35m:f32[6288,512]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rj\n",
       "                ss\u001b[35m:f32[64,8,64]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rk\n",
       "                st\u001b[35m:f32[64,8,64]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rl\n",
       "                su\u001b[35m:f32[64,8,64]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rm\n",
       "                sv\u001b[35m:f32[1,64,512]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rn\n",
       "                sw\u001b[35m:f32[64,2048]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] ro\n",
       "                sx\u001b[35m:f32[256,512]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rp\n",
       "                sy\u001b[35m:f32[64,50304]\u001b[39m = reduce_scatter[\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  scatter_dimension=0\n",
       "                  tiled=True\n",
       "                ] rq\n",
       "                sz\u001b[35m:i32[]\u001b[39m = mul sq 6288:i32[]\n",
       "                ta\u001b[35m:bool[]\u001b[39m = lt sz 0:i32[]\n",
       "                tb\u001b[35m:i32[]\u001b[39m = add sz 50304:i32[]\n",
       "                tc\u001b[35m:i32[]\u001b[39m = select_n ta sz tb\n",
       "                td\u001b[35m:f32[6288,512]\u001b[39m = dynamic_slice[slice_sizes=(6288, 512)] si tc 0:i32[]\n",
       "                te\u001b[35m:i32[]\u001b[39m = mul sq 64:i32[]\n",
       "                tf\u001b[35m:bool[]\u001b[39m = lt te 0:i32[]\n",
       "                tg\u001b[35m:i32[]\u001b[39m = add te 512:i32[]\n",
       "                th\u001b[35m:i32[]\u001b[39m = select_n tf te tg\n",
       "                ti\u001b[35m:f32[64,8,64]\u001b[39m = dynamic_slice[slice_sizes=(64, 8, 64)] sj th 0:i32[]\n",
       "                  0:i32[]\n",
       "                tj\u001b[35m:i32[]\u001b[39m = mul sq 64:i32[]\n",
       "                tk\u001b[35m:bool[]\u001b[39m = lt tj 0:i32[]\n",
       "                tl\u001b[35m:i32[]\u001b[39m = add tj 512:i32[]\n",
       "                tm\u001b[35m:i32[]\u001b[39m = select_n tk tj tl\n",
       "                tn\u001b[35m:f32[64,8,64]\u001b[39m = dynamic_slice[slice_sizes=(64, 8, 64)] sk tm 0:i32[]\n",
       "                  0:i32[]\n",
       "                to\u001b[35m:i32[]\u001b[39m = mul sq 64:i32[]\n",
       "                tp\u001b[35m:bool[]\u001b[39m = lt to 0:i32[]\n",
       "                tq\u001b[35m:i32[]\u001b[39m = add to 512:i32[]\n",
       "                tr\u001b[35m:i32[]\u001b[39m = select_n tp to tq\n",
       "                ts\u001b[35m:f32[64,8,64]\u001b[39m = dynamic_slice[slice_sizes=(64, 8, 64)] sl tr 0:i32[]\n",
       "                  0:i32[]\n",
       "                tt\u001b[35m:i32[]\u001b[39m = mul sq 1:i32[]\n",
       "                tu\u001b[35m:bool[]\u001b[39m = lt tt 0:i32[]\n",
       "                tv\u001b[35m:i32[]\u001b[39m = add tt 8:i32[]\n",
       "                tw\u001b[35m:i32[]\u001b[39m = select_n tu tt tv\n",
       "                tx\u001b[35m:f32[1,64,512]\u001b[39m = dynamic_slice[slice_sizes=(1, 64, 512)] sm tw\n",
       "                  0:i32[] 0:i32[]\n",
       "                ty\u001b[35m:i32[]\u001b[39m = mul sq 64:i32[]\n",
       "                tz\u001b[35m:bool[]\u001b[39m = lt ty 0:i32[]\n",
       "                ua\u001b[35m:i32[]\u001b[39m = add ty 512:i32[]\n",
       "                ub\u001b[35m:i32[]\u001b[39m = select_n tz ty ua\n",
       "                uc\u001b[35m:f32[64,2048]\u001b[39m = dynamic_slice[slice_sizes=(64, 2048)] sn ub 0:i32[]\n",
       "                ud\u001b[35m:i32[]\u001b[39m = mul sq 256:i32[]\n",
       "                ue\u001b[35m:bool[]\u001b[39m = lt ud 0:i32[]\n",
       "                uf\u001b[35m:i32[]\u001b[39m = add ud 2048:i32[]\n",
       "                ug\u001b[35m:i32[]\u001b[39m = select_n ue ud uf\n",
       "                uh\u001b[35m:f32[256,512]\u001b[39m = dynamic_slice[slice_sizes=(256, 512)] so ug 0:i32[]\n",
       "                ui\u001b[35m:i32[]\u001b[39m = mul sq 64:i32[]\n",
       "                uj\u001b[35m:bool[]\u001b[39m = lt ui 0:i32[]\n",
       "                uk\u001b[35m:i32[]\u001b[39m = add ui 512:i32[]\n",
       "                ul\u001b[35m:i32[]\u001b[39m = select_n uj ui uk\n",
       "                um\u001b[35m:f32[64,50304]\u001b[39m = dynamic_slice[slice_sizes=(64, 50304)] sp ul 0:i32[]\n",
       "                un\u001b[35m:f32[6288,512]\u001b[39m = integer_pow[y=1] sr\n",
       "                uo\u001b[35m:f32[6288,512]\u001b[39m = mul 0.10000000149011612:f32[] un\n",
       "                up\u001b[35m:f32[6288,512]\u001b[39m = mul 0.8999999761581421:f32[] rs\n",
       "                uq\u001b[35m:f32[6288,512]\u001b[39m = add uo up\n",
       "                ur\u001b[35m:f32[64,8,64]\u001b[39m = integer_pow[y=1] ss\n",
       "                us\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.10000000149011612:f32[] ur\n",
       "                ut\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.8999999761581421:f32[] rt\n",
       "                uu\u001b[35m:f32[64,8,64]\u001b[39m = add us ut\n",
       "                uv\u001b[35m:f32[64,8,64]\u001b[39m = integer_pow[y=1] st\n",
       "                uw\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.10000000149011612:f32[] uv\n",
       "                ux\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.8999999761581421:f32[] ru\n",
       "                uy\u001b[35m:f32[64,8,64]\u001b[39m = add uw ux\n",
       "                uz\u001b[35m:f32[64,8,64]\u001b[39m = integer_pow[y=1] su\n",
       "                va\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.10000000149011612:f32[] uz\n",
       "                vb\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.8999999761581421:f32[] rv\n",
       "                vc\u001b[35m:f32[64,8,64]\u001b[39m = add va vb\n",
       "                vd\u001b[35m:f32[1,64,512]\u001b[39m = integer_pow[y=1] sv\n",
       "                ve\u001b[35m:f32[1,64,512]\u001b[39m = mul 0.10000000149011612:f32[] vd\n",
       "                vf\u001b[35m:f32[1,64,512]\u001b[39m = mul 0.8999999761581421:f32[] rw\n",
       "                vg\u001b[35m:f32[1,64,512]\u001b[39m = add ve vf\n",
       "                vh\u001b[35m:f32[64,2048]\u001b[39m = integer_pow[y=1] sw\n",
       "                vi\u001b[35m:f32[64,2048]\u001b[39m = mul 0.10000000149011612:f32[] vh\n",
       "                vj\u001b[35m:f32[64,2048]\u001b[39m = mul 0.8999999761581421:f32[] rx\n",
       "                vk\u001b[35m:f32[64,2048]\u001b[39m = add vi vj\n",
       "                vl\u001b[35m:f32[256,512]\u001b[39m = integer_pow[y=1] sx\n",
       "                vm\u001b[35m:f32[256,512]\u001b[39m = mul 0.10000000149011612:f32[] vl\n",
       "                vn\u001b[35m:f32[256,512]\u001b[39m = mul 0.8999999761581421:f32[] ry\n",
       "                vo\u001b[35m:f32[256,512]\u001b[39m = add vm vn\n",
       "                vp\u001b[35m:f32[64,50304]\u001b[39m = integer_pow[y=1] sy\n",
       "                vq\u001b[35m:f32[64,50304]\u001b[39m = mul 0.10000000149011612:f32[] vp\n",
       "                vr\u001b[35m:f32[64,50304]\u001b[39m = mul 0.8999999761581421:f32[] rz\n",
       "                vs\u001b[35m:f32[64,50304]\u001b[39m = add vq vr\n",
       "                vt\u001b[35m:f32[6288,512]\u001b[39m = integer_pow[y=2] sr\n",
       "                vu\u001b[35m:f32[6288,512]\u001b[39m = mul 0.0010000000474974513:f32[] vt\n",
       "                vv\u001b[35m:f32[6288,512]\u001b[39m = mul 0.9990000128746033:f32[] sa\n",
       "                vw\u001b[35m:f32[6288,512]\u001b[39m = add vu vv\n",
       "                vx\u001b[35m:f32[64,8,64]\u001b[39m = integer_pow[y=2] ss\n",
       "                vy\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.0010000000474974513:f32[] vx\n",
       "                vz\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.9990000128746033:f32[] sb\n",
       "                wa\u001b[35m:f32[64,8,64]\u001b[39m = add vy vz\n",
       "                wb\u001b[35m:f32[64,8,64]\u001b[39m = integer_pow[y=2] st\n",
       "                wc\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.0010000000474974513:f32[] wb\n",
       "                wd\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.9990000128746033:f32[] sc\n",
       "                we\u001b[35m:f32[64,8,64]\u001b[39m = add wc wd\n",
       "                wf\u001b[35m:f32[64,8,64]\u001b[39m = integer_pow[y=2] su\n",
       "                wg\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.0010000000474974513:f32[] wf\n",
       "                wh\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.9990000128746033:f32[] sd\n",
       "                wi\u001b[35m:f32[64,8,64]\u001b[39m = add wg wh\n",
       "                wj\u001b[35m:f32[1,64,512]\u001b[39m = integer_pow[y=2] sv\n",
       "                wk\u001b[35m:f32[1,64,512]\u001b[39m = mul 0.0010000000474974513:f32[] wj\n",
       "                wl\u001b[35m:f32[1,64,512]\u001b[39m = mul 0.9990000128746033:f32[] se\n",
       "                wm\u001b[35m:f32[1,64,512]\u001b[39m = add wk wl\n",
       "                wn\u001b[35m:f32[64,2048]\u001b[39m = integer_pow[y=2] sw\n",
       "                wo\u001b[35m:f32[64,2048]\u001b[39m = mul 0.0010000000474974513:f32[] wn\n",
       "                wp\u001b[35m:f32[64,2048]\u001b[39m = mul 0.9990000128746033:f32[] sf\n",
       "                wq\u001b[35m:f32[64,2048]\u001b[39m = add wo wp\n",
       "                wr\u001b[35m:f32[256,512]\u001b[39m = integer_pow[y=2] sx\n",
       "                ws\u001b[35m:f32[256,512]\u001b[39m = mul 0.0010000000474974513:f32[] wr\n",
       "                wt\u001b[35m:f32[256,512]\u001b[39m = mul 0.9990000128746033:f32[] sg\n",
       "                wu\u001b[35m:f32[256,512]\u001b[39m = add ws wt\n",
       "                wv\u001b[35m:f32[64,50304]\u001b[39m = integer_pow[y=2] sy\n",
       "                ww\u001b[35m:f32[64,50304]\u001b[39m = mul 0.0010000000474974513:f32[] wv\n",
       "                wx\u001b[35m:f32[64,50304]\u001b[39m = mul 0.9990000128746033:f32[] sh\n",
       "                wy\u001b[35m:f32[64,50304]\u001b[39m = add ww wx\n",
       "                wz\u001b[35m:bool[]\u001b[39m = lt rr 2147483647:i32[]\n",
       "                xa\u001b[35m:i32[]\u001b[39m = add rr 1:i32[]\n",
       "                xb\u001b[35m:i32[]\u001b[39m = jit[\n",
       "                  name=_where\n",
       "                  ctx_mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; wz\u001b[35m:bool[]\u001b[39m xa\u001b[35m:i32[]\u001b[39m xc\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
       "                      \u001b[39;22mxb\u001b[35m:i32[]\u001b[39m = select_n wz xc xa\n",
       "                    \u001b[34;1min \u001b[39;22m(xb,) }\n",
       "                ] wz xa 2147483647:i32[]\n",
       "                _\u001b[35m:f32[]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=True] xb\n",
       "                xd\u001b[35m:f32[]\u001b[39m = pow 0.9:f32[] xb\n",
       "                xe\u001b[35m:f32[]\u001b[39m = sub 1.0:f32[] xd\n",
       "                xf\u001b[35m:f32[6288,512]\u001b[39m = div uq xe\n",
       "                xg\u001b[35m:f32[64,8,64]\u001b[39m = div uu xe\n",
       "                xh\u001b[35m:f32[64,8,64]\u001b[39m = div uy xe\n",
       "                xi\u001b[35m:f32[64,8,64]\u001b[39m = div vc xe\n",
       "                xj\u001b[35m:f32[1,64,512]\u001b[39m = div vg xe\n",
       "                xk\u001b[35m:f32[64,2048]\u001b[39m = div vk xe\n",
       "                xl\u001b[35m:f32[256,512]\u001b[39m = div vo xe\n",
       "                xm\u001b[35m:f32[64,50304]\u001b[39m = div vs xe\n",
       "                _\u001b[35m:f32[]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=True] xb\n",
       "                xn\u001b[35m:f32[]\u001b[39m = pow 0.999:f32[] xb\n",
       "                xo\u001b[35m:f32[]\u001b[39m = sub 1.0:f32[] xn\n",
       "                xp\u001b[35m:f32[6288,512]\u001b[39m = div vw xo\n",
       "                xq\u001b[35m:f32[64,8,64]\u001b[39m = div wa xo\n",
       "                xr\u001b[35m:f32[64,8,64]\u001b[39m = div we xo\n",
       "                xs\u001b[35m:f32[64,8,64]\u001b[39m = div wi xo\n",
       "                xt\u001b[35m:f32[1,64,512]\u001b[39m = div wm xo\n",
       "                xu\u001b[35m:f32[64,2048]\u001b[39m = div wq xo\n",
       "                xv\u001b[35m:f32[256,512]\u001b[39m = div wu xo\n",
       "                xw\u001b[35m:f32[64,50304]\u001b[39m = div wy xo\n",
       "                xx\u001b[35m:f32[6288,512]\u001b[39m = add xp 0.0:f32[]\n",
       "                xy\u001b[35m:f32[6288,512]\u001b[39m = sqrt xx\n",
       "                xz\u001b[35m:f32[6288,512]\u001b[39m = add xy 9.99999993922529e-09:f32[]\n",
       "                ya\u001b[35m:f32[6288,512]\u001b[39m = div xf xz\n",
       "                yb\u001b[35m:f32[64,8,64]\u001b[39m = add xq 0.0:f32[]\n",
       "                yc\u001b[35m:f32[64,8,64]\u001b[39m = sqrt yb\n",
       "                yd\u001b[35m:f32[64,8,64]\u001b[39m = add yc 9.99999993922529e-09:f32[]\n",
       "                ye\u001b[35m:f32[64,8,64]\u001b[39m = div xg yd\n",
       "                yf\u001b[35m:f32[64,8,64]\u001b[39m = add xr 0.0:f32[]\n",
       "                yg\u001b[35m:f32[64,8,64]\u001b[39m = sqrt yf\n",
       "                yh\u001b[35m:f32[64,8,64]\u001b[39m = add yg 9.99999993922529e-09:f32[]\n",
       "                yi\u001b[35m:f32[64,8,64]\u001b[39m = div xh yh\n",
       "                yj\u001b[35m:f32[64,8,64]\u001b[39m = add xs 0.0:f32[]\n",
       "                yk\u001b[35m:f32[64,8,64]\u001b[39m = sqrt yj\n",
       "                yl\u001b[35m:f32[64,8,64]\u001b[39m = add yk 9.99999993922529e-09:f32[]\n",
       "                ym\u001b[35m:f32[64,8,64]\u001b[39m = div xi yl\n",
       "                yn\u001b[35m:f32[1,64,512]\u001b[39m = add xt 0.0:f32[]\n",
       "                yo\u001b[35m:f32[1,64,512]\u001b[39m = sqrt yn\n",
       "                yp\u001b[35m:f32[1,64,512]\u001b[39m = add yo 9.99999993922529e-09:f32[]\n",
       "                yq\u001b[35m:f32[1,64,512]\u001b[39m = div xj yp\n",
       "                yr\u001b[35m:f32[64,2048]\u001b[39m = add xu 0.0:f32[]\n",
       "                ys\u001b[35m:f32[64,2048]\u001b[39m = sqrt yr\n",
       "                yt\u001b[35m:f32[64,2048]\u001b[39m = add ys 9.99999993922529e-09:f32[]\n",
       "                yu\u001b[35m:f32[64,2048]\u001b[39m = div xk yt\n",
       "                yv\u001b[35m:f32[256,512]\u001b[39m = add xv 0.0:f32[]\n",
       "                yw\u001b[35m:f32[256,512]\u001b[39m = sqrt yv\n",
       "                yx\u001b[35m:f32[256,512]\u001b[39m = add yw 9.99999993922529e-09:f32[]\n",
       "                yy\u001b[35m:f32[256,512]\u001b[39m = div xl yx\n",
       "                yz\u001b[35m:f32[64,50304]\u001b[39m = add xw 0.0:f32[]\n",
       "                za\u001b[35m:f32[64,50304]\u001b[39m = sqrt yz\n",
       "                zb\u001b[35m:f32[64,50304]\u001b[39m = add za 9.99999993922529e-09:f32[]\n",
       "                zc\u001b[35m:f32[64,50304]\u001b[39m = div xm zb\n",
       "                zd\u001b[35m:f32[6288,512]\u001b[39m = mul 0.009999999776482582:f32[] td\n",
       "                ze\u001b[35m:f32[6288,512]\u001b[39m = add ya zd\n",
       "                zf\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.009999999776482582:f32[] ti\n",
       "                zg\u001b[35m:f32[64,8,64]\u001b[39m = add ye zf\n",
       "                zh\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.009999999776482582:f32[] tn\n",
       "                zi\u001b[35m:f32[64,8,64]\u001b[39m = add yi zh\n",
       "                zj\u001b[35m:f32[64,8,64]\u001b[39m = mul 0.009999999776482582:f32[] ts\n",
       "                zk\u001b[35m:f32[64,8,64]\u001b[39m = add ym zj\n",
       "                zl\u001b[35m:f32[1,64,512]\u001b[39m = mul 0.009999999776482582:f32[] tx\n",
       "                zm\u001b[35m:f32[1,64,512]\u001b[39m = add yq zl\n",
       "                zn\u001b[35m:f32[64,2048]\u001b[39m = mul 0.009999999776482582:f32[] uc\n",
       "                zo\u001b[35m:f32[64,2048]\u001b[39m = add yu zn\n",
       "                zp\u001b[35m:f32[256,512]\u001b[39m = mul 0.009999999776482582:f32[] uh\n",
       "                zq\u001b[35m:f32[256,512]\u001b[39m = add yy zp\n",
       "                zr\u001b[35m:f32[64,50304]\u001b[39m = mul 0.009999999776482582:f32[] um\n",
       "                zs\u001b[35m:f32[64,50304]\u001b[39m = add zc zr\n",
       "                zt\u001b[35m:f32[6288,512]\u001b[39m = mul -9.999999747378752e-05:f32[] ze\n",
       "                zu\u001b[35m:f32[64,8,64]\u001b[39m = mul -9.999999747378752e-05:f32[] zg\n",
       "                zv\u001b[35m:f32[64,8,64]\u001b[39m = mul -9.999999747378752e-05:f32[] zi\n",
       "                zw\u001b[35m:f32[64,8,64]\u001b[39m = mul -9.999999747378752e-05:f32[] zk\n",
       "                zx\u001b[35m:f32[1,64,512]\u001b[39m = mul -9.999999747378752e-05:f32[] zm\n",
       "                zy\u001b[35m:f32[64,2048]\u001b[39m = mul -9.999999747378752e-05:f32[] zo\n",
       "                zz\u001b[35m:f32[256,512]\u001b[39m = mul -9.999999747378752e-05:f32[] zq\n",
       "                baa\u001b[35m:f32[64,50304]\u001b[39m = mul -9.999999747378752e-05:f32[] zs\n",
       "                bab\u001b[35m:f32[50304,512]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zt\n",
       "                bac\u001b[35m:f32[512,8,64]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zu\n",
       "                bad\u001b[35m:f32[512,8,64]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zv\n",
       "                bae\u001b[35m:f32[512,8,64]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zw\n",
       "                baf\u001b[35m:f32[8,64,512]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zx\n",
       "                bag\u001b[35m:f32[512,2048]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zy\n",
       "                bah\u001b[35m:f32[2048,512]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] zz\n",
       "                bai\u001b[35m:f32[512,50304]\u001b[39m = all_gather[\n",
       "                  all_gather_dimension=0\n",
       "                  axis_index_groups=None\n",
       "                  axis_name=('data',)\n",
       "                  axis_size=8\n",
       "                  tiled=True\n",
       "                ] baa\n",
       "              \u001b[34;1min \u001b[39;22m(bab, bac, bad, bae, baf, bag, bah, bai, xb, uq, uu, uy, vc, vg,\n",
       "                vk, vo, vs, vw, wa, we, wi, wm, wq, wu, wy) }\n",
       "            manual_axes=frozenset({'data'})\n",
       "            mesh=Mesh('data': 8, axis_types=(Explicit,))\n",
       "            out_specs=(PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec(), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',), PartitionSpec('data',))\n",
       "          ] pv pw px py pz qa qb qc i qd qe qf qg qh qi qj qk ql qm qn qo qp qq qr\n",
       "            qs qt qu qv qw qx qy qz ra\n",
       "          bd\u001b[35m:f32[50304,512]\u001b[39m = add a rb\n",
       "          be\u001b[35m:f32[512,8,64]\u001b[39m = add b rc\n",
       "          bf\u001b[35m:f32[512,8,64]\u001b[39m = add c rd\n",
       "          bg\u001b[35m:f32[512,8,64]\u001b[39m = add d re\n",
       "          bh\u001b[35m:f32[8,64,512]\u001b[39m = add e rf\n",
       "          bi\u001b[35m:f32[512,2048]\u001b[39m = add f rg\n",
       "          bj\u001b[35m:f32[2048,512]\u001b[39m = add g rh\n",
       "          bk\u001b[35m:f32[512,50304]\u001b[39m = add h ri\n",
       "        \u001b[34;1min \u001b[39;22m(bd, be, bf, bg, bh, bi, bj, bk, bl, bm, bn, bo, bp, bq, br, bs, bt, bu,\n",
       "          bv, bw, bx, by, bz, ca, cb, cc) }\n",
       "    ] a b c d e f g h i j k l m n o p q r s t u v w x y z ba bb bc\n",
       "  \u001b[34;1min \u001b[39;22m(bd, be, bf, bg, bh, bi, bj, bk, bl, bm, bn, bo, bp, bq, br, bs, bt, bu, bv,\n",
       "    bw, bx, by, bz, ca, cb, cc) }"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(train_step)(model_weights, optimizer_state, cos, sin, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac448284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, _, loss = train_step(model_weights, optimizer_state, cos, sin, x, y)\n",
    "print()\n",
    "_, _, loss = train_step(model_weights, optimizer_state, cos, sin, x, y)\n",
    "print()\n",
    "_, _, loss = train_step(model_weights, optimizer_state, cos, sin, x, y)\n",
    "print()\n",
    "_, _, loss = train_step(model_weights, optimizer_state, cos, sin, x, y)\n",
    "print()\n",
    "_, _, loss = train_step(model_weights, optimizer_state, cos, sin, x, y)\n",
    "print()\n",
    "# with jax.profiler.trace(\"f_profiles\"):\n",
    "#     _, _, loss = train_step(model_weights, optimizer_state, cos, sin, x, y)\n",
    "#     loss.block_until_ready()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc4ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
